{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd14ba6-8884-40df-bccf-4f1157424d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .lower()\n",
    "text = \"Hello, World!\"\n",
    "lower_text = text.lower()\n",
    "lower_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec504186-f984-467d-ab31-753e69b8f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .upper()\n",
    "text = \"Hello, World!\"\n",
    "upper_text = text.upper()\n",
    "upper_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667f53a-bc0d-4977-b8cf-31742fbc6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .strip() to remove whitespace\n",
    "text = \"  extra spaces  \"\n",
    "stripped_text = text.strip()\n",
    "stripped_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97474e36-77c8-4f40-99a3-4722e3202951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .strip() to remove a string\n",
    "text = \"  extra spaces!?!?\"\n",
    "stripped_text = text.strip(\"!?!?\")\n",
    "stripped_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186e244-89b4-4b00-8919-51c94224e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .split() on whitespaces\n",
    "text = \"apple, orange, and banana\"\n",
    "split_text = text.split()\n",
    "split_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d90c9-9cea-4070-abcc-3b549c8e7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .split() on whitespaces\n",
    "text = \"apple, orange, and banana\"\n",
    "split_text = text.split(\",\")\n",
    "split_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfd596-b90d-4e78-aba3-9777b007e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .join()\n",
    "words = ['apple', 'orange', 'banana']\n",
    "joined_text = \", \".join(words)\n",
    "joined_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd46ae-062b-4d76-a9c7-d1cccec11e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .replace()\n",
    "text = \"I like apples.\"\n",
    "new_text = text.replace(\"apples\", \"oranges\")\n",
    "new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d5192-9438-47c9-b843-81346c6d5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .startswith()\n",
    "text = \"Hello, World!\"\n",
    "starts_with = text.startswith(\"Hello\")\n",
    "starts_with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ad5c5-451c-44e0-b1d8-03fb861acb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .endswith()\n",
    "text = \"Hello, World!\"\n",
    "ends_with = text.endswith(\"World!\")\n",
    "ends_with\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047ef0f-7134-45d4-8440-181ef776a04f",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac192ec6-cbc8-45cb-af15-9a4498ac4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0c7c1-773d-4928-9b23-33aec02e5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Contact us at support@example.com and sales@example2.com for more info.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504969d-8a42-42d8-b9dc-3f6b7b203b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = 'us'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95fb9a-71ab-4913-bb9e-7b82677198f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'[aeiou]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a8f03-ea84-4d27-bc7d-a7c7d7b41684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'[c-f]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8ee94-b15f-483d-aad8-10006e76c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6096599-3bcd-4a17-8d67-6f4ee5b4b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98da98-eea0-4c0f-b917-e09527f303ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2b640-a0c1-4cf6-8811-db2e373b5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a49d83-e73b-4b8b-ad26-3b6f8fdbb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]+@'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4694141-c1ae-4ca8-ac3b-aeac76e8bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b18fa7-452d-44ae-bf76-9c69cb46c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with improper email and non-email \"@\"\n",
    "text = \"Contact us at support@example.com and sales@example2.com and help@tech for more info. We look forward to seeing you@5pm!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c02dd5-54ce-44e2-a792-a6588ddf5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603f593-674d-41d5-be46-74f67ec1dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe1fea-0d33-46f6-9250-22e5d57c390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d74066-51dc-4502-9bb9-70c51a66ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid and invalid emails\n",
    "email_test = 'ninja@codingdojo.com, ninjacodingdojo.com, ninja@codingdojo, ninja@codingdojo.y'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682bbe0-0c19-428d-8fe6-928bc22cc9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, email_test)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087264d-3671-480a-882f-04c3bf1ca205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern - FINAL Pattern for Identifying emails\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, email_test)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966f523-cf38-47af-82c4-236fba26f655",
   "metadata": {},
   "source": [
    "## Summary\r\n",
    "In this lesson, we introduced regular expressions by demonstrating the building and testing of a pattern to identify emails. Regex patterns can look quite complex at first glance, but breaking them down into smaller pieces can help you recognize what each element in the pattern is achieving. Developing the pattern often requires an iterative process with testing. As we demonstrated, you should consider including edge cases and non-examples in your test text to ensure the pattern will work as expected.\r\n",
    "\r\n",
    "\n",
    "\n",
    "Resources\r\n",
    "www.regexr.com\r\n",
    "\r\n",
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d275be-fdd5-4aa9-b1d3-9777d930fd2f",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772f3a5-fa5d-446a-826d-e7a1e4382bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax\n",
    "with open(fpath) as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fa732-3629-48e3-8627-7a4a445a7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Windows\n",
    "with open(fpath, encoding = \"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd0803-92b4-4f74-a2a4-007b7ed91d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Mac\n",
    "with open(fpath, encoding = \"cp1252\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa600d55-daf6-4343-9441-c9a525c831a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1,000 characters (printing)\n",
    "print(txt[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79c3c6-2019-4119-90bd-2570bf934253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b5b8a-394b-4b29-9cd3-89b971d2442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(random_state = 123).generate(txt)\n",
    "plt.imshow(cloud);\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c6e23-fdac-4836-a74c-c97b834fb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "STOPWORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79beaac-5c91-462b-b9c3-9ca6121a2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom stopwords\n",
    "custom_stopwords = [\"said\",'Alice','s', *STOPWORDS]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c84fc6-1851-4386-b3af-0e330d296c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(random_state=123, stopwords=custom_stopwords).generate(txt)\n",
    "plt.imshow(cloud);\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a46e2e-8800-4ea3-bd86-72dbc0190a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different color map\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    colormap=\"plasma\"\n",
    ").generate(txt)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63573097-60f2-49db-804c-edfd5e47eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"plasma\",\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c8272-1b70-4f05-be46-ec1571774b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"plasma\",\n",
    "    width = 800,\n",
    "    height = 400\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02878749-60e1-4979-b1db-c777e0445e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "    max_words=200,\n",
    "    colormap=\"plasma\",\n",
    "    min_word_length=2,\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbdf81-777a-45d7-a04a-b54ffa2e1ef7",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc785-0195-479b-bc11-7bca6fb6d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text for exploration\n",
    "sample_text = \"Hey there, Ninjas! I'm so excited to tell you about NLP. It's super-cool, isn't it? I've been #learning a lot. Follow @NLP for updates. E-mail me at fake_address@email.com\"\n",
    "sample_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a67e4-3900-4173-bfdd-cd0cb61543c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect lowercase text\n",
    "sample_text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f86dff-03ff-457c-babb-5fab0665ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace Tokenization\n",
    "whitespace_tokens = sample_text.lower().split()\n",
    "# Print tokens\n",
    "print('Whitespace tokens: \\n',whitespace_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39777146-f67c-4c41-b7fa-7c51469e177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To more easily see each token, display instead of print\n",
    "whitespace_tokens[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9406e-9b10-4cc3-bb91-6a7861bcb4d6",
   "metadata": {},
   "source": [
    "One very common NLP library is NLTK (Natural Language Toolkit).\r\n",
    "## \r\n",
    "Important Note: NLTK Downloads\r\n",
    "\r\n",
    "​You will encounter error messages the first time you try to run several tools from NLTK. The error message has the exact commands you need to run to download the required component. \r\n",
    "\r\n",
    " It will use the  \"nltk.download()\" function, and the error message tells you what to use as the argument for nltk.download. \r\n",
    "\r\n",
    "e.g. \"nltk.download('punkt')\"\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb9e4b-14ce-4aff-8259-73a64848ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# NLTK's Word Tokenization\n",
    "word_tokens = word_tokenize(sample_text.lower())\n",
    "print(\"Original text: \\n\", sample_text, '\\n\\n')\n",
    "print('Word tokens: \\n', word_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88c4cf-5c7c-45b5-b064-a19f7190bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# NLTK's Tweet Tokenization\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sample_text.lower())\n",
    "print(\"Original text: \\n\", sample_text, '\\n\\n')\n",
    "print(f\"Tweet Tokens: \\n\", tweet_tokens,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f64f5-ab6b-4e14-a2ff-85c14f7e73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "# Define bigrams\n",
    "bigrams = ngrams(tweet_tokens, 2)\n",
    "# display bigrams\n",
    "list(bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a6096-f32e-40e1-b93a-07b3051aada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trigrams\n",
    "trigrams = ngrams(tweet_tokens, 3)\n",
    "# display trigrams\n",
    "list(trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fff929-97fc-4280-88c3-2ba90888183b",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dd166-d35c-453c-b13c-a030f94289e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windows\n",
    "# Define the filepath\n",
    "fpath = 'Data/alice_in_wonderland.txt'\n",
    "# Use with Open syntax\n",
    "with open(fpath, encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "print(f\"there are {len(txt)} characters in the full text.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe1da3-4706-4fa6-8810-cc58eb7c6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mac\n",
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Mac\n",
    "with open(fpath, encoding = \"cp1252\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0579cc-b790-46eb-ac5d-47b506488622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = txt.lower().split()\n",
    "tokens[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121bcc5-e825-436a-8014-2ba8f6587cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06364490-0aa2-4b69-b165-9074e9c1d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd115a-636f-4cbd-9d73-f94ddcea5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.plot(20, title='Frequency of Words in Alices in Wonderland')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe45a62-5526-4212-89ff-009196dd4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.plot(20, title='Frequency of Words in Alices in Wonderland', percents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855313b-4baf-45d9-a6c6-21b76cb37a3f",
   "metadata": {},
   "source": [
    "Finally, we can use matplotlib to manipulate these plots, but ONLY if we set 'show=False' when calling the .plot() method. In this case, the .plot() method will return a matplotlib ax object. This would be useful, for example, if you wanted to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569988f-941f-4b5e-9942-2cc3f577e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = dist.plot(20, show=False)\n",
    "ax.set_title('Number of Occurances of Top 20 Words')\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('frequency_distribution.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c7356-4ed4-48fe-b694-c2a1a9989122",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b39bd-af8f-45bb-bf5e-69dfe0bf002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Example text\n",
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "words =  sample_text.split(' ')\n",
    "print(\"Original Words:\\n\", words,'\\n\\n')\n",
    "# Applying stemming\n",
    "stemmed_words = [stemmer.stem(w.lower()) for w in words]\n",
    "print(\"Stemmed Words:\\n\", stemmed_words,'\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e32d6-8883-48f4-8e01-2eea2990f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Original Words:\\n\", words,'\\n')\n",
    "# Lemmatizing example text\n",
    "lemmas = [lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "print(\"Lemmas:\\n\", lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0653e-10e5-4ed8-94da-55cec1676688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing 'running' as a verb\n",
    "print(\"As a verb:\", lemmatizer.lemmatize('running', pos='v'))\n",
    "# Lemmatizing 'running' as a noun\n",
    "print(\"As a noun:\", lemmatizer.lemmatize('running', pos='n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd9b8b-3608-40b9-9777-9919c1386057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing 'was' as default(noun)\n",
    "print(\"As default(noun):\", lemmatizer.lemmatize('was'))\n",
    "# Lemmatizing 'was' as a verb\n",
    "print(\"As verb:\", lemmatizer.lemmatize('was', pos='v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963c201-88a5-40f6-b3d3-d3ab34536117",
   "metadata": {},
   "source": [
    "## NLP with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a56-aaec-49ee-b334-1b555a63b70f",
   "metadata": {},
   "source": [
    "Official 101 to SpaCy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Installing SpaCy\r\n",
    "\r\n",
    "Use spaCy's usage page and enter your OS/system info to get the installation commands for your computer.\r\n",
    "We want:\r\n",
    "Package Manager: Conda\r\n",
    "Hardware: CPU\r\n",
    "Trained Pipelines: English\r\n",
    "Select Model for: Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bd879-9dd6-4ed8-b4f1-9ae5b81f2942",
   "metadata": {},
   "source": [
    "(In your terminal)\r\n",
    "\r\n",
    "conda install -c conda-forge spacy\r\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc883490-b1f6-41c5-9563-7ad191af9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741f775-2a04-4862-af35-0b417d6fa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in the pipeline\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbca72a-f2c5-4487-b86d-ec876ce2edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable ner\n",
    "nlp_no_ner = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "# Print active components\n",
    "nlp_no_ner.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeabc94-7be9-446c-a358-266d39e7fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text for demonstration\n",
    "sample_text = \"While running in Central Park, \\nI noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\"\n",
    "print(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618138fc-f18f-4374-86bd-7f169d42037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc with the nlp pipeline\n",
    "doc = nlp(sample_text)\n",
    "type(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335dbc8-2a2b-428d-acf2-d4ed86a40ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bec12-9d5c-43ec-8364-34dd26602f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 10 tokens separately\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ff1c0-dac7-404e-9e33-065fbe87c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a token from the doc\n",
    "token = doc[1]\n",
    "token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969d91a-ec75-44b1-b5a4-f3e161f14646",
   "metadata": {},
   "source": [
    "a) token.text: The original form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c4ed9-d741-47fb-9f98-acc128c47ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662bbae-973e-41a3-bc19-9ecf781de1cb",
   "metadata": {},
   "source": [
    "b) token.lemma_: The base or root form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0d0e4-5402-4d1b-877a-bb4b4bc2da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e88f0-d5d3-47d0-9da8-70f7f08880d6",
   "metadata": {},
   "source": [
    "c) token.pos_: The part-of-speech tag associated with the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d843df1-6a49-4a96-ad32-0523ef16e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.pos_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1043e5-b11b-4c0a-bbff-1303085d295a",
   "metadata": {},
   "source": [
    "d) token.is_stop: Boolean flag to check if the token is a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca368bc3-ebfe-4167-b01d-b59a62efe3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88554d5-be68-43ca-a99c-57c562c0e2ba",
   "metadata": {},
   "source": [
    "e) token.is_punct: Boolean flag to check if the token is punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd230c-a92c-4e02-863a-9e3caedf7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_punct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a349f84-a0f4-43f9-8dc1-494bb457233c",
   "metadata": {},
   "source": [
    "f) token.is_space: Boolean flag to check if the token is a whitespace character (.e.g new line \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c72c53-bf3e-43a5-b610-dc432a377957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63ae63-aad8-47bb-a77a-660eccf7a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create dictionary for desired attributes for each token\n",
    "token_data = []\n",
    "for token in doc:\n",
    "    token_dict = {\n",
    "        \".text\": token.text,\n",
    "        \".lemma_\": token.lemma_,\n",
    "        \".pos_\": token.pos_,\n",
    "        \".is_stop\": token.is_stop,\n",
    "        \".is_punct\": token.is_punct,\n",
    "        \".is_space\": token.is_space\n",
    "    }\n",
    "    token_data.append(token_dict)\n",
    "# Save dictionary as a dataframe\n",
    "spacy_df = pd.DataFrame(token_data) \n",
    "spacy_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076179b1-f0be-4f1f-af61-b52e4ca4dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to remove stopwords\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword, skip it\n",
    "    if token.is_stop == True:\n",
    "        continue \n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "print(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81580578-3736-4662-877a-9c18b2f93db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    ##NEW: \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "        \n",
    "print(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20e35b-e577-4b99-8dca-a81b52f9cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_lemmas = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # # keep the tokens'.text for the final list of tokens\n",
    "        # cleaned_tokens.append(token.text.lower())\n",
    "        # keep the tokens's .lemma_ for the final list of tokens\n",
    "        cleaned_lemmas.append(token.lemma_.lower())\n",
    "        \n",
    "print(cleaned_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7149549-4741-4243-a475-55e061fba8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text and lemmas\n",
    "print(\"Tokenized words:\\n\", cleaned_tokens,\"\\n\")\n",
    "print(\"Lemmatized words:\\n\", cleaned_lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b7afe-e80a-4a9e-98b5-18dc69ee52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc, remove_stopwords=True, remove_punct=True, use_lemmas=False):\n",
    "    \"\"\"Temporary Fucntion - for Education Purposes (we will make something better below)\n",
    "    \"\"\"\n",
    "    tokens = [ ]\n",
    "    for token in doc:\n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_stopwords == True) and (token.is_stop == True):\n",
    "            # Continue the loop with the next token\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_punct == True):\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_space == True):\n",
    "            continue\n",
    "    \n",
    "        ## Determine final form of output list of tokens/lemmas\n",
    "        if use_lemmas:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e7e3b-5372-4fd9-9b4b-a1019857afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to a doc.\n",
    "doc = nlp(sample_text)\n",
    "# Tokenizing, keeping stopwords and punctuatin\n",
    "dirty_tokens = preprocess_doc(doc, remove_stopwords=False,remove_punct=False)\n",
    "print(dirty_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef2396-527a-4cf7-8a4d-12b22878b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, removing stopwords and punctuation\n",
    "cleaned_tokens = preprocess_doc(doc, remove_stopwords=True,remove_punct=True)\n",
    "print(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67558f-ce5c-4e57-8c00-1ad7da70a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing, removing stopwords and punctuation\n",
    "cleaned_lemmas = preprocess_doc(doc, remove_stopwords=True,remove_punct=True, use_lemmas=True)\n",
    "print(cleaned_lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d02343-a5bb-402e-8457-a8dabad89ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Example Framework (Not runnable)\n",
    "lists_of_texts = [text1, text2, text3]\n",
    "processed_texts = []\n",
    "for doc in nlp.pipe(list_of_texts):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        # ... the same logic from our preprocess docs function.\n",
    "        doc_tokens.append(token.text.lower())\n",
    "        \n",
    "    # Append the list of tokens for current doc to processed_texts\n",
    "    processed_texts.append(doc_tokens)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0aab4-2ab9-4243-8c5e-e52123dcf1cc",
   "metadata": {},
   "source": [
    "# Be sure to save this function. You will add it to your custom_functions.py file in an upcoming lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd04304-7d76-4d80-97da-add3aaa4ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    processed_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44dc11-53ed-410d-bc71-6bc478b1a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default args will produce tokens\n",
    "tokens = batch_preprocess_texts([sample_text])\n",
    "tokens = tokens[0]\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b0df2-2173-43d0-bace-08be9a6b83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting use_lemmas = True will produce lemmas\n",
    "lemmas = batch_preprocess_texts([sample_text], use_lemmas=True)\n",
    "lemmas = lemmas[0]\n",
    "print(lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8978026-a5dd-4ae5-b849-cc5e22331e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "doc = nlp(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73023316-06bd-4c50-be83-4eec756a2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from doc\n",
    "sentences = list(doc.sents)\n",
    "len(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e443-4632-45e3-a28d-ef1e0ffb30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first sentence\n",
    "sentences[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6688f1-e4a5-4432-ac89-4afdfef619ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print any named entities in the doc and its label\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04aa494-0323-4271-a682-a9bc09d663de",
   "metadata": {},
   "source": [
    "# Text in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d755dc-71aa-4846-af90-b222d836d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4d51d-5543-42a1-9932-719a0be03c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "​def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deacead-21c9-402a-9d44-763f77894576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_businesses = pd.read_csv(\"Data/yelp-business-metadata.csv.gz\", index_col='business_id')\n",
    "df_businesses.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac8411-1ad4-4703-bc96-16c82748209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the reviews for CA 2015-2018\n",
    "df = pd.read_csv('Data/yelp-reviews-CA-2015-2018.csv.gz', index_col='review_id')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255c2a0-b6e6-4037-b694-1b90397a7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New pandas option to change: \n",
    "pd.get_option('display.max_colwidth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbebe3-a11c-42c0-9e0d-35f610742ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase column width\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb049b-9233-4a1c-ad78-36cfabafaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using city\n",
    "filter_city = df_businesses['city'].str.contains('Santa Barbara') \n",
    "filter_city.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc62dc-4bf0-4742-a65c-2fd3421e7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using name\n",
    "filter_name =  df_businesses['name'].str.contains(\"Sandbar\")\n",
    "filter_name.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b9217-6e56-4067-898a-ec452190cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the target business\n",
    "selected_business =df_businesses.loc[ filter_name & filter_city]\n",
    "selected_business\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bcf7a-d458-4df5-9d41-2d7bff0dbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the business id for slicing the reviews\n",
    "business_id = selected_business.index[0]\n",
    "business_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e547763-f349-4b9a-8fde-9ec1ec8f74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep reviews for the selected business\n",
    "reviews = df.loc[ df['business_id']==business_id]\n",
    "reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd343b5f-a238-4fba-9518-dbc4a52fbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample review\n",
    "sample_review = reviews.iloc[0]\n",
    "sample_review['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e6a5c-311d-439c-a8d1-16e8c7730434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each star rating?\n",
    "sns.countplot(data = reviews, x = 'stars');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c98b7c-798b-4397-84ee-d06d74c7b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to only 1 and 5 star reviews\n",
    "reviews = reviews[reviews['stars'].isin([1,5])]\n",
    "reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe94dfc-7609-4334-a68e-0a837920d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What languages are represented?\n",
    "reviews['language'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b8655-71a3-4133-a558-3f7fa9f86ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to only English\n",
    "reviews = reviews[reviews['language']=='en']\n",
    "reviews['language'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d0a2f-0d9e-479e-92ea-ae071d76d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many 1 and 5 star reviews?\n",
    "reviews['stars'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f831-b489-426d-b242-0b81577911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop business_id and language\n",
    "reviews = reviews.drop(columns=['business_id', 'language'])\n",
    "# Make data a datetime object\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f30e4-a24a-4ffb-b5f2-c4439f99be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Disable parser and ner\n",
    "nlp_light = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])\n",
    "# Print active components\n",
    "nlp_light.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3413f1-3ed1-491f-ad28-61c42c919ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch preprocess the text and store tokens\n",
    "reviews['tokens'] = batch_preprocess_texts(reviews['text'], nlp = nlp_light)\n",
    "reviews.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb5af7-446a-4fb4-bce5-4c37cbbb78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch preprocess the text and store lemmas\n",
    "reviews['lemmas'] = batch_preprocess_texts(reviews['text'], nlp = nlp_light, use_lemmas = True)\n",
    "reviews.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77a428-dd03-48e8-ac02-9fe5df871d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of tokens\n",
    "sample_review = reviews.iloc[0]\n",
    "sample_review['tokens']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdab23-8103-4199-a7dc-4ffd66028c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data type of tokens\n",
    "type(sample_review['tokens'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ca54c-dee2-42ef-9ca2-b82ca8ecaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data type of lemmas\n",
    "type(sample_review['lemmas'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c82bc8-1e9d-4633-9207-49f8209976c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join list of tokens into a string with spaces between each token\n",
    "reviews['tokens-joined'] = reviews['tokens'].map(lambda x: \" \".join(x))\n",
    "# Join list of lemmas into a string with spaces between each lemma\n",
    "reviews['lemmas-joined'] = reviews['lemmas'].map(lambda x: \" \".join(x))\n",
    "reviews.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130acafc-368c-440b-a5e4-a7def5e2e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first review as sample\n",
    "sample_review = reviews.iloc[0]\n",
    "# confirm data type of tokens-joined\n",
    "print(type(sample_review['tokens-joined']))\n",
    "# confirm data type of lemmas-joined\n",
    "print(type(sample_review['lemmas-joined']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7d351-0ca0-40cd-807e-245c29f07407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the tokens-joined into a single string\n",
    "sample_review['tokens-joined']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a4a4d-a44f-408e-851a-37ea19ad5bed",
   "metadata": {},
   "source": [
    "## Comparing Groups - Word Clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8feca-0a9f-4879-ad80-e39aafbfdc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset from previous lesson\n",
    "reviews.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc3a73-a86c-414b-9073-f31ead25e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters for 1 and 5 star reviews\n",
    "filter_high = reviews['stars'] == 5\n",
    "filter_low = reviews['stars'] == 1\n",
    "filter_high.sum(), filter_low.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4f5ee-db24-4b96-9eee-a2bfece290e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4448d-cb6a-4f48-a3fe-d47e0f9bf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star reviews\n",
    "high_reviews_text = \" \".join( reviews.loc[filter_high, 'text'])\n",
    "print(high_reviews_text[:1000],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5a17e-764a-4bbe-a79d-7175cde31523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 1 star reviews\n",
    "low_reviews_text = \" \".join( reviews.loc[filter_low, 'text'])\n",
    "print(low_reviews_text[:1000],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa875f-5983-4c87-ac46-9c65d1e1aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of raw text\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_text)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_text)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Raw Reviews', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d16806-cfe1-4722-b864-3220284dac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star tokens\n",
    "high_reviews_tokens = \" \".join( reviews.loc[filter_high, 'tokens-joined'])\n",
    "# Make a single giant string with entire group of 1 star tokens\n",
    "low_reviews_tokens = \" \".join( reviews.loc[filter_low, 'tokens-joined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ca36e-ea38-411b-87c5-5ac3d8776e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of processed tokens\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_tokens)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_tokens)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Tokens', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3962fb-322e-44c8-8f23-af193cfa9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star lemmas\n",
    "high_reviews_lemmas = \" \".join( reviews.loc[filter_high, 'lemmas-joined'])\n",
    "# Make a single giant string with entire group of 1 star lemmas\n",
    "low_reviews_lemmas= \" \".join( reviews.loc[filter_low, 'lemmas-joined'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225a0cb-d6b9-4322-96fe-b224cb353269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of lemmas\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_lemmas)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_lemmas)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Lemmas', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25bbc6-975c-4860-9c19-96964601ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "custom_stopwords = ['good', 'great', 'Sandbar', 'Santa', 'Barbara', 'place', 'come', 'drink']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581019a6-6be6-4d7c-ac6b-95d5d24c5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of lemmas with custom stopwords\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                      stopwords = custom_stopwords\n",
    "                     ).generate(low_reviews_lemmas)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                       stopwords = custom_stopwords\n",
    "                      ).generate(high_reviews_lemmas)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Lemmas', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239fb89-e3f9-4c9c-9405-d805fda8ca74",
   "metadata": {},
   "source": [
    "## Comparing Groups: Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf118c19-bab7-4a49-8901-3d3855e53360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Freqdist\n",
    "from nltk.probability import FreqDist\n",
    "# Split the lemmas into individual token words\n",
    "low_review_lemmas_split = low_reviews_lemmas.split()\n",
    "# Pass the tokenized lemmas to the class constructor and plot the distribution \n",
    "low_dist = FreqDist(low_review_lemmas_split)\n",
    "ax = low_dist.plot(20, show = False, title='Distribution of Words in One-Star Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('low_review_freq_dist.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e9efc-b4ea-47a3-b9cb-514b9302af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lemmas into individual token words\n",
    "high_review_lemmas_split = high_reviews_lemmas.split()\n",
    "# Pass the tokenized lemmas to the class constructor and plot the distribution \n",
    "low_dist = FreqDist(high_review_lemmas_split)\n",
    "ax = low_dist.plot(20, show=False, title='Distribution of Words in Five-Star Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('high_review_freq_dist.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ad33b-b191-427e-a97a-7775d61caf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lemmas in high reviews\n",
    "exploded_high_review_lemmas = reviews.loc[filter_high, 'lemmas'].explode()\n",
    "exploded_high_review_lemmas.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495e32c-1d49-4c4b-ab87-3ebc41788e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lemmas in low reviews\n",
    "exploded_low_review_lemmas = reviews.loc[filter_low, 'lemmas'].explode()\n",
    "exploded_low_review_lemmas.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5528a8-0015-4e62-a6b7-efc4678a3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of all lemmas in both high and low review groups\n",
    "high_review_lemmas_list = reviews.loc[filter_high, 'lemmas'].explode().to_list()\n",
    "low_review_lemmas_list = reviews.loc[filter_low, 'lemmas'].explode().to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d4d0a-dc67-43e1-9e4a-d7edab0fd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of words in high reviews\n",
    "high_dist = FreqDist(high_review_lemmas_list)\n",
    "high_dist.plot(20, title='Distribution of Words in 5 Star Reviews')\n",
    "# Plot distribution of words in low reviews\n",
    "low_dist = FreqDist(low_review_lemmas_list)\n",
    "low_dist.plot(20, title='Distribution of Words in 1 Star Reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac561c-cf2f-463a-887f-4b0fd6ab2630",
   "metadata": {},
   "source": [
    "## Comping Groups: N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96704c-27ce-4681-909c-4d84a11e08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1000 tokens in high reviews\n",
    "high_reviews_tokens[: 1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6baa0-40cd-43c6-83de-b204440b0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1000 tokens in low reviews\n",
    "low_reviews_tokens[: 1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895863b1-6090-4445-8b4d-6b54f93016e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split single string into individual list elements\n",
    "high_reviews_tokens_split = high_reviews_tokens.split()\n",
    "high_reviews_tokens_split[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc700216-a0a8-48d9-be83-9eae3df88546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split single string into individual list elements\n",
    "low_reviews_tokens_split = low_reviews_tokens.split()\n",
    "low_reviews_tokens_split[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fd577-7a2c-4898-b049-c0cd283a9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview our dataframe\n",
    "reviews.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1393d6-2e15-406f-87e0-24b014cba7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain single list of tokens from all 5 star reviews\n",
    "high_reviews_tokens_exploded_list = reviews.loc[filter_high, 'tokens'].explode().to_list()\n",
    "high_reviews_tokens_exploded_list[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09102359-9fc3-401e-a0b0-888398a39348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain single list of tokens from all 1 star reviews\n",
    "low_reviews_tokens_exploded_list = reviews.loc[filter_low, 'tokens'].explode().to_list()\n",
    "low_reviews_tokens_exploded_list[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12766079-8c42-4db5-85b7-f1cebf55647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Instantiate a measures objects for Bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3d9dd-bb89-4e4c-b16a-701e2a6f03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a finder for the high reviews\n",
    "bigram_finder_high = nltk.BigramCollocationFinder.from_words(high_reviews_tokens_exploded_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350d3d1-d987-4bdd-aa57-db375e580b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scores for the bigrams using the score_ngrams method \n",
    "# Include the desired scoring method of the measures object\n",
    "bigrams_scores_high = bigram_finder_high.score_ngrams(bigram_measures.raw_freq)\n",
    "bigrams_scores_high[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b6814-2002-47cc-9e0f-8613bbaca2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a finder for the low reviews\n",
    "bigram_finder_low = nltk.BigramCollocationFinder.from_words(low_reviews_tokens_exploded_list)\n",
    "\n",
    "# Obtain scores for the bigrams using the score_ngrams method \n",
    "# Include the desired scoring method of the measures object\n",
    "bigrams_scores_low = bigram_finder_low.score_ngrams(bigram_measures.raw_freq)\n",
    "bigrams_scores_low[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eeb602-55f9-47b9-856d-587cc84ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to dataframe\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_scores_high, columns=['Words','Frequency'])\n",
    "df_bigram_scores_high.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb55b0b-349c-4d79-b81c-7e0c19216a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to dataframe\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_scores_low, columns=['Words','Frequency'])\n",
    "df_bigram_scores_low.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83621f2d-4abe-41c1-a47a-b5323dc4fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine low and high reviews score dfs and add a group name as multi-index\n",
    "df_compare_bigrams = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ff26-70dc-42ef-9ccb-f68f91a70465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the pmi score with the score_ngrams\n",
    "bigrams_pmi_high = bigram_finder_high.score_ngrams(bigram_measures.pmi)\n",
    "# Repeat for low reviews\n",
    "bigrams_pmi_low = bigram_finder_low.score_ngrams(bigram_measures.pmi)\n",
    "# Preview results\n",
    "bigrams_pmi_high[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331da81-9219-4d5c-9b73-c1aef4956387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for high review pmi scores\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_pmi_high, columns=['Words','PMI Score'])\n",
    "\n",
    "# df for low review pmi scores\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_pmi_low, columns=['Words','PMI Score'])\n",
    "\n",
    "\n",
    "# Combine both groups and add a group name as multi-index\n",
    "df_compare_bigrams_pmi = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams_pmi.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4903c18-02ad-420c-a82d-75bd8dbcc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired minimum frequency value\n",
    "min_frequency = 3 \n",
    "# Apply filter to finder for the high reviews\n",
    "bigram_finder_high.apply_freq_filter(min_frequency)\n",
    "# Apply filter to finder for the low reviews\n",
    "bigram_finder_low.apply_freq_filter(min_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cc29e-01a3-4506-b8cb-10b338f6a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the pmi score with the score_ngrams\n",
    "bigrams_pmi_high = bigram_finder_high.score_ngrams(bigram_measures.pmi)\n",
    "# Repeat for low reviews\n",
    "bigrams_pmi_low = bigram_finder_low.score_ngrams(bigram_measures.pmi)\n",
    "# Preview results\n",
    "bigrams_pmi_high[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad07f8f-4237-441e-9b60-0adef2d62541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for high review pmi scores\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_pmi_high, columns=['Words','PMI Score'])\n",
    "\n",
    "# df for low review pmi scores\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_pmi_low, columns=['Words','PMI Score'])\n",
    "\n",
    "\n",
    "# Combine both groups and add a group name as multi-index\n",
    "df_compare_bigrams_pmi = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams_pmi.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9b2ca-17ad-4011-b206-e3f48a565aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075db07-af98-4853-b51f-d5302f5cb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tri-grams\n",
    "nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# For quad-grams\n",
    "nltk.collocations.QuadgramAssocMeasures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac06d2-bc7f-4527-ac71-a956f0f01b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.collocations.BigramCollocationFinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd8cdd-a939-4f34-a2f8-e763f98297b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tri-grams\n",
    "nltk.collocations.TrigramCollocationFinder\n",
    "\n",
    "# For quad-grams\n",
    "nltk.collocations.QuadgramCollocationFinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b7679-4247-4795-a46a-bde2c2ed5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_ngram_measures_finder(tokens, ngrams=2, measure='raw_freq', top_n=None, min_freq = 1,\n",
    "                             words_colname='Words'):\n",
    "    import nltk\n",
    "    if ngrams == 4:\n",
    "        MeasuresClass = nltk.collocations.QuadgramAssocMeasures\n",
    "        FinderClass = nltk.collocations.QuadgramCollocationFinder\n",
    "        \n",
    "    elif ngrams == 3: \n",
    "        MeasuresClass = nltk.collocations.TrigramAssocMeasures\n",
    "        FinderClass = nltk.collocations.TrigramCollocationFinder\n",
    "    else:\n",
    "        MeasuresClass = nltk.collocations.BigramAssocMeasures\n",
    "        FinderClass = nltk.collocations.BigramCollocationFinder\n",
    "\n",
    "    measures = MeasuresClass()\n",
    "    \n",
    "   \n",
    "    finder = FinderClass.from_words(tokens)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    if measure=='pmi':\n",
    "        scored_ngrams = finder.score_ngrams(measures.pmi)\n",
    "    else:\n",
    "        measure='raw_freq'\n",
    "        scored_ngrams = finder.score_ngrams(measures.raw_freq)\n",
    "\n",
    "    df_ngrams = pd.DataFrame(scored_ngrams, columns=[words_colname, measure.replace(\"_\",' ').title()])\n",
    "    if top_n is not None:\n",
    "        return df_ngrams.head(top_n)\n",
    "    else:\n",
    "        return df_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6e35e-d60a-47be-9062-af5fe4813a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function to find frequency values for quad-grams in low reviews\n",
    "get_ngram_measures_finder(low_reviews_tokens_exploded_list, ngrams=4, top_n=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39a7f8-1413-4f2c-86f8-c65a9a1bd526",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385b99c-4b09-40f2-a9fd-71dd36375d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration texts\n",
    "sample_df = pd.DataFrame([\n",
    "    'Pizza is good',\n",
    "    'Pizza is very good',\n",
    "    'Pizza is great',\n",
    "    'Pizza is great!',\n",
    "    'Pizza is great :-)',\n",
    "    'Pizza is not bad',\n",
    "    'Pizza is bad',\n",
    "    'Pizza is bad!',\n",
    "    'Pizza is bad!!!',\n",
    "    'Pizza is a bomb',\n",
    "    'Pizza is the bomb'], columns = ['text'])\n",
    "sample_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a7a22-4105-46f5-a2ed-2429558abd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "# The download below only needs to be completed once\n",
    "nltk.download('vader_lexicon')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb1ccc-347c-4159-a704-e53ef54a904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab8d3b-b46d-41f8-b700-94a946c41762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['sentiment_scores'] = sample_df['text'].map(sia.polarity_scores)\n",
    "# Convert the column of dictionaries into separate columns\n",
    "sample_df_scores = sample_df['sentiment_scores'].apply(pd.Series)\n",
    "# Join the new DataFrame with the original DataFrame\n",
    "sample_df = pd.concat([sample_df, sample_df_scores], axis=1).drop('sentiment_scores', axis=1)\n",
    "sample_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e765a-bb71-4328-b761-bdbfb42f171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b66614-a522-4d10-9c4b-ecd7c174d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment scores to the reviews df\n",
    "reviews['sentiment_scores'] = reviews['text'].map(sia.polarity_scores)\n",
    "# Convert the column of dictionaries into separate columns\n",
    "review_scores = reviews['sentiment_scores'].apply(pd.Series)\n",
    "# Join the new DataFrame with the original DataFrame\n",
    "reviews = pd.concat([reviews, review_scores], axis=1).drop('sentiment_scores', axis=1)\n",
    "reviews.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d064d-18bf-4a2b-af9b-7fefc2792213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filter for positive reviews\n",
    "pos_sentiment_reviews = reviews['compound'] > 0\n",
    "pos_sentiment_reviews.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35612110-7659-423f-a5f2-b6dcdcaafa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify reviews with a positive sentiment score that received 1 star\n",
    "filter_mismatch_A = (reviews['stars'] == 1) & pos_sentiment_reviews\n",
    "pos_sent_1_star = reviews.loc[filter_mismatch_A,['text','stars','compound']]\n",
    "pos_sent_1_star.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b5114-6e2c-449d-a2e4-4b6f262cd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent_1_star.loc['nD1jsduurtESBlToiYCBZw']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdf282-36be-417a-af38-1906b1dfdc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify 5 star reviews with negative sentiment\n",
    "filter_mismatch_B = (reviews['stars'] == 5) & ~pos_sentiment_reviews\n",
    "reviews.loc[filter_mismatch_B,['text','stars','compound']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ccfaa-7586-49f2-85c1-a7d5749da43b",
   "metadata": {},
   "source": [
    "# Customizing nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69ecda-bb87-40d1-9315-648f53363baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8aa2b-7f69-4c9c-8145-8b61333d353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample text\n",
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. I don't like flies, but I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "sample_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b4b1-4e7d-4688-a919-a27dc6e5386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove stopwords\n",
    "tokens_keep_all_stop = preprocess_text(sample_text, remove_stopwords = False)\n",
    "print(tokens_keep_all_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afccd03-bd01-43b2-889b-c5d8c0353cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove default stopwords\n",
    "tokens_remove_default_stop = preprocess_text(sample_text)\n",
    "print(tokens_remove_default_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a8423-c5b7-4366-afa5-9ba2a2426cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looop to find words removed\n",
    "removed_tokens = []\n",
    "for token in tokens_keep_all_stop:\n",
    "    if token not in tokens_remove_default_stop:\n",
    "        removed_tokens.append(token)\n",
    "removed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7b907-6470-4691-bb71-1dbcac74ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom nlp pipeline\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "# Let's start by accessing spaCy's default stopwords\n",
    "spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "spacy_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04595cca-303c-4cd7-898b-ac0ed72aff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many default stopwords?\n",
    "len(spacy_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c9b17-3fbd-43f2-bd9d-2289a285630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can include additional stopwords by adding them to the default set\n",
    "# Add custom stopwords\n",
    "custom_stopwords = [\"food\", \"likely\",'upset','carelessly']\n",
    "for word in custom_stopwords:\n",
    "    # Add the word to the list of stopwords (for easily tracking stopwords)\n",
    "    custom_nlp.Defaults.stop_words.add(word)\n",
    "    # Set the is_stop attribute for the word in the vocab dict to true. \n",
    "    # this is what will actually determine spacy treating the word as a stop word\n",
    "    custom_nlp.vocab[word].is_stop = True\n",
    "updated_spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "len(updated_spacy_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791a6f6-5b41-4b0b-abc7-4173f266c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "remove_stopwords = [\"but\", \"someone\"]\n",
    "for word in remove_stopwords:\n",
    "    custom_nlp.Defaults.stop_words.discard(word)\n",
    "    # Ensure the words are not recognized as stopwords\n",
    "    custom_nlp.vocab[word].is_stop = False\n",
    "updated_spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "len(updated_spacy_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9bb19-c25b-4834-b111-a3944cce4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text with custom nlp pipeline\n",
    "custom_stopwords_removed = preprocess_text(sample_text, nlp = custom_nlp)\n",
    "print(custom_stopwords_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df720429-9913-487c-8117-98c41269b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of contractions to keep as single tokens\n",
    "contractions = [\"don't\", \"couldn't\"]\n",
    "# Loop through the contractions list and add special cases\n",
    "for contraction in contractions:\n",
    "    special_case = [{\"ORTH\": contraction}]\n",
    "    custom_nlp.tokenizer.add_special_case(contraction, special_case)\n",
    "keep_contractions = preprocess_text(sample_text, nlp = custom_nlp)\n",
    "print(keep_contractions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137d66c-1c87-41a1-be40-c459162110c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_nlp(\n",
    "    disable=[\"ner\"],\n",
    "    contractions=[\"don't\", \"can't\", \"couldn't\", \"you'd\", \"I'll\"],\n",
    "    stopwords_to_add=[],\n",
    "    stopwords_to_remove=[],\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "):\n",
    "    \"\"\"Returns a custom spacy nlp pipeline.\n",
    "    \n",
    "    Args:\n",
    "        disable (list, optional): Names of pipe components to disable. Defaults to [\"ner\"].\n",
    "        contractions (list, optional): List of contractions to add as special cases. Defaults to [\"don't\", \"can't\", \"couldn't\", \"you'd\", \"I'll\"].\n",
    "        stopwords_to_add(list, optional): List of words to set as stopwords (word.is_stop=True)\n",
    "        stopwords_to_remove(list, optional): List of words to remove from stopwords (word.is_stop=False)\n",
    "        spacy_model(string, optional): String to select a spacy language model. (Defaults to \"en_core_web_sm\".)\n",
    "                            Additional Options:  \"en_core_web_md\", \"en_core_web_lg\"; \n",
    "                            (Must first download the model by name in the terminal:\n",
    "                            e.g.  \"python -m spacy download en_core_web_lg\" )\n",
    "            \n",
    "    Returns:\n",
    "        nlp pipeline: spacy pipeline with special cases and updated nlp.Default.stopwords\n",
    "    \"\"\"\n",
    "    # Load the English NLP model\n",
    "    nlp = spacy.load(spacy_model, disable=disable)\n",
    "    \n",
    "    # Adding Special Cases \n",
    "    # Loop through the contractions list and add special cases\n",
    "    for contraction in contractions:\n",
    "        special_case = [{\"ORTH\": contraction}]\n",
    "        nlp.tokenizer.add_special_case(contraction, special_case)\n",
    "    \n",
    "    # Adding stopwords\n",
    "    for word in stopwords_to_add:\n",
    "        # Set the is_stop attribute for the word in the vocab dict to true.\n",
    "        nlp.vocab[\n",
    "            word\n",
    "        ].is_stop = True  # this determines spacy's treatmean of the word as a stop word\n",
    "        # Add the word to the list of stopwords (for easily tracking stopwords)\n",
    "        nlp.Defaults.stop_words.add(word)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    for word in stopwords_to_remove:\n",
    "        \n",
    "        # Ensure the words are not recognized as stopwords\n",
    "        nlp.vocab[word].is_stop = False\n",
    "        nlp.Defaults.stop_words.discard(word)\n",
    "        \n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254c3d4-412b-4de9-ab0c-8d8fda586844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the nlp pipeline\n",
    "function_nlp = make_custom_nlp(    \n",
    "    disable=['ner', 'parser'],\n",
    "    contractions=[\"don't\"],\n",
    "    stopwords_to_add=['park'],\n",
    "    stopwords_to_remove=['while'],\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    ")\n",
    "# call preprocessing function with custom nlp pipeline\n",
    "tokens = batch_preprocess_texts([sample_text], nlp = function_nlp)\n",
    "print(tokens[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d58f1-d079-4b1c-98c3-c03f96c8d74c",
   "metadata": {},
   "source": [
    "# Scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41362d3a-0f93-446c-b832-8682c98d2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d344a-4fdd-4f4b-b577-5c76ff706d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "import scattertext as st\n",
    "from scattertext import (\n",
    "    SampleCorpora,\n",
    "    produce_scattertext_explorer,\n",
    "    produce_scattertext_html,\n",
    ")\n",
    "from scattertext.CorpusFromPandas import CorpusFromPandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459ed6a-a7bd-4dcc-84b4-1077b3927308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "reviews.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e68c2b-4ac3-4c11-9020-588c48c0e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nlp pipeline \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67420d92-b6ae-48cd-bed7-99ac1f9fc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the scattertext corpus object\n",
    "corpus = st.CorpusFromPandas(\n",
    "    reviews, category_col=\"stars\",\n",
    "    text_col='text', \n",
    "    nlp=nlp\n",
    ").build().remove_terms(nlp.Defaults.stop_words, ignore_absences = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefa6b3-e590-4d24-bc65-d4c083bb077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccd700-ed75-42bc-846b-ed9bfe57565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the terms found in the reviews that are different than general English texts\n",
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61850a9-b290-4b0b-ba6a-68c713a4586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get term frequency df \n",
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755f3e2-014a-4029-8fbd-c960f0010570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the f score for each term for each term\n",
    "term_freq_df['5-star'] = corpus.get_scaled_f_scores('5')\n",
    "term_freq_df['1-star'] = corpus.get_scaled_f_scores('1')\n",
    "\n",
    "term_freq_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6165c3-ca15-479e-b539-199527a8a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms most associated with 5 star reviews\n",
    "print(list(term_freq_df.sort_values(by='5-star', ascending=False).index[:10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd161e96-5584-4ca1-969b-c090e5bcaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms most associated with 1 star reviews\n",
    "print(list(term_freq_df.sort_values(by='1-star', ascending=False).index[:10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12050b01-e699-4f94-95aa-f8cd5e729f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create html visualization\n",
    "# Provide file path to save \n",
    "scatter_fname = './scattertext_restaurant.html'\n",
    "\n",
    "scatter_html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category= '5',\n",
    "    category_name=\"5-Star Reviews\",\n",
    "    not_category_name=\"1-Star Reviews\",\n",
    "    minimum_term_frequency=3,\n",
    "    width_in_pixels=1000,\n",
    "    metadata=reviews[\"text\"],\n",
    ")\n",
    "open(scatter_fname, 'wb').write(scatter_html.encode('utf-8'))\n",
    "print(f'Open {scatter_fname} in Chrome or Firefox.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161ab99-bd58-4f0b-833f-e5d7a82dbd35",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0320b9-14b9-4ef1-9077-d7976fe2353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1a869-5fea-46cd-aeb3-715a6ac45eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "X = np.array([\n",
    "    \"I enjoy learning new programming languages. The best is Python. Programming is so fun!\",\n",
    "    \"I love programming, I would give it an A+!\",\n",
    "    \"Programming is amazing. Programming is love. Programming is life.\",\n",
    "    \"Python is my favorite programming language.\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751883c-965e-4304-9765-b1a71fc191ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "X = np.array([\n",
    "    \"I enjoy learning new programming languages. The best is Python. Programming is so fun!\",\n",
    "    \"I love programming, I would give it an A+!\",\n",
    "    \"Programming is amazing. Programming is love. Programming is life.\",\n",
    "    \"Python is my favorite programming language.\"\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f805923-3639-49f8-bca9-fdeb7ae4bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate a vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit it on the data \n",
    "vectorizer.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42461b43-2200-4584-b2ba-ff2551134202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate a vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit it on the data \n",
    "vectorizer.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cf6a7-22d9-4116-bc00-a1dd2a94398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves vocab - matches number of columns above\n",
    "vocab_dict = vectorizer.vocabulary_\n",
    "type(vocab_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835b3ac-0667-42ae-8500-6ed602a8e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words in our vocab?\n",
    "len(vocab_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b198ad-3e1a-45ac-a1d0-4d4eedf4bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504e3bb-3ff5-4932-bd0e-8a0ad189fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the count, transform the X data\n",
    "X_count = vectorizer.transform(X)\n",
    "type(X_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1aecde-8bba-47a4-9dea-bf85ec341121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to array for display\n",
    "X_count.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b670f-b202-4db7-92cd-302ef60e56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the array\n",
    "X_count.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384cc35-e64e-4e72-b186-5494875eaebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array into a df\n",
    "X_count_df = pd.DataFrame(X_count.toarray(), columns= vectorizer.get_feature_names_out())\n",
    "X_count_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b4a23-e367-4134-aa76-1153f8fa176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfVectorizer Example\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns= tfidf_vectorizer.get_feature_names_out())\n",
    "X_tfidf_df.round(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd0227-ccb8-48dd-8b2b-120518791899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "vectorizer_stopped = CountVectorizer(stop_words='english')\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_stopped.fit_transform(X)\n",
    "X_stopped = pd.DataFrame(X_vec.toarray(), columns= vectorizer_stopped.get_feature_names_out())\n",
    "X_stopped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d4c7d-7b40-4599-9f7d-ae743f7fb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing default vocab \n",
    "print(f\"# of terms in original vocabulary: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"# of terms in stopwords-removed vocabulary: {len(vectorizer_stopped.vocabulary_)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac88e9-4d5d-472c-83ca-2308524ac9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "vectorizer_tfidf_stopped = TfidfVectorizer(stop_words='english')\n",
    "# Fit it on the data \n",
    "X_vec_tfidf_stopped = vectorizer_tfidf_stopped.fit_transform(X)\n",
    "X_stopped_tfidf = pd.DataFrame(X_vec_tfidf_stopped.toarray(), columns= vectorizer_tfidf_stopped.get_feature_names_out())\n",
    "X_stopped_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2521653-81da-4fd3-9e47-d67b5e0de7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "custom_stopwords = [*ENGLISH_STOP_WORDS, 'programming']\n",
    "# instantiate a vectorizer\n",
    "vectorizer_stopped_custom = CountVectorizer(stop_words=custom_stopwords)\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_stopped_custom.fit_transform(X)\n",
    "X_stopped_custom = pd.DataFrame(X_vec.toarray(), columns= vectorizer_stopped_custom.get_feature_names_out())\n",
    "X_stopped_custom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f6ae3-c7fd-466f-b309-c87d6ef209ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "# instantiate a vectorizer with english stopwords\n",
    "vectorizer_nltk = CountVectorizer(stop_words='english',\n",
    "                                  tokenizer=wordpunct_tokenize, token_pattern = None)\n",
    "# Fit it on the data \n",
    "X_count_nltk = vectorizer_nltk.fit_transform(X)\n",
    "# Getting the feature names (vocabulary)\n",
    "X_count_nltk_df = pd.DataFrame(X_count_nltk.toarray(), columns= vectorizer_nltk.get_feature_names_out())\n",
    "X_count_nltk_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2312e34-d1b4-4275-b090-18847c8ed7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "# instantiate a vectorizer with english stopwords\n",
    "vectorizer_nltk = TfidfVectorizer(stop_words='english',\n",
    "                                  tokenizer=wordpunct_tokenize, token_pattern = None)\n",
    "# Fit it on the data \n",
    "X_count_nltk = vectorizer_nltk.fit_transform(X)\n",
    "# Getting the feature names (vocabulary)\n",
    "X_count_nltk_df = pd.DataFrame(X_count_nltk.toarray(), columns= vectorizer_nltk.get_feature_names_out())\n",
    "X_count_nltk_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a491df9-8394-4599-b1ab-7ac4752ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer to include bigrams\n",
    "vectorizer_ngrams = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_ngrams.fit_transform(X)\n",
    "X_ngrams = pd.DataFrame(X_vec.toarray(), columns= vectorizer_ngrams.get_feature_names_out())\n",
    "X_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7f629-edcf-412b-bca4-15779eee3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer to include bigrams and trigrams\n",
    "vectorizer_ngrams = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_ngrams.fit_transform(X)\n",
    "X_ngrams = pd.DataFrame(X_vec.toarray(), columns= vectorizer_ngrams.get_feature_names_out())\n",
    "X_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fc7fc-eeae-4e27-82ac-9c1c216966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer_ngrams.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c660342-23bb-45ac-a1e4-ff50f1ed4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "vectorizer_max10 = CountVectorizer(stop_words='english', max_features=10)\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_max10.fit_transform(X)\n",
    "X_max10 = pd.DataFrame(X_vec.toarray(), columns= vectorizer_max10.get_feature_names_out())\n",
    "X_max10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45d0a0-42f6-4f5f-9a7e-c252eee5927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "vectorizer_maxdf = CountVectorizer(stop_words='english', max_df = .5)\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_maxdf.fit_transform(X)\n",
    "X_maxdf = pd.DataFrame(X_vec.toarray(), columns= vectorizer_maxdf.get_feature_names_out())\n",
    "X_maxdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee222a44-b4b5-4314-bada-7a078b5d4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "vectorizer_mindf = CountVectorizer(stop_words='english', min_df = .5)\n",
    "# Fit it on the data \n",
    "X_vec = vectorizer_mindf.fit_transform(X)\n",
    "X_mindf = pd.DataFrame(X_vec.toarray(), columns= vectorizer_mindf.get_feature_names_out())\n",
    "X_mindf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f644f-9704-4e74-b505-8530531b5bbe",
   "metadata": {},
   "source": [
    "# Text Classification with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e45b7-a4ec-4576-ad3f-77c06c6f0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_colwidth',300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba40724-4158-4830-baa3-5868aac66603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "    \n",
    "    \n",
    "def evaluate_classification(model, X_train, y_train, X_test, y_test,\n",
    "                         figsize=(6,4), normalize='true', output_dict = False,\n",
    "                            cmap_train='Blues', cmap_test=\"Reds\",colorbar=False):\n",
    "  # Get predictions for training data\n",
    "  y_train_pred = model.predict(X_train)\n",
    "  # Call the helper function to obtain regression metrics for training data\n",
    "  results_train = classification_metrics(y_train, y_train_pred, #verbose = verbose,\n",
    "                                     output_dict=True, figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_train,\n",
    "                                     label='Training Data')\n",
    "  print()\n",
    "  # Get predictions for test data\n",
    "  y_test_pred = model.predict(X_test)\n",
    "  # Call the helper function to obtain regression metrics for test data\n",
    "  results_test = classification_metrics(y_test, y_test_pred, #verbose = verbose,\n",
    "                                  output_dict=True,figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_test,\n",
    "                                    label='Test Data' )\n",
    "  if output_dict == True:\n",
    "    # Store results in a dataframe if ouput_frame is True\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85413c5-6b64-49eb-937c-c1176773cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data using your path\n",
    "df = pd.read_csv(\"Data/Yelp/subset/yelp-restaurant-reviews-CA-2015-2018.csv.gz\", index_col='review_id')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12802009-c2f2-46dd-b174-588e2ca84965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain languages\n",
    "df['language'].value_counts(dropna=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9024d-0406-4832-884d-bb0f4e552d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any non-english reivews\n",
    "df = df.loc[ df['language']=='en'].copy()\n",
    "len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e255f7-3a5e-4927-a0af-278a53bebffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce sample size for demonstration\n",
    "df = df.sample(n=20_000, random_state=42)\n",
    "len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221ddbc-7f9f-479f-801a-6e6574dfe847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to only 1,3, or 5 star reviews\n",
    "df = df[df['stars'].isin([1,3,5])]\n",
    "# Get value couunts for stars column\n",
    "val_counts = df['stars'].value_counts()\n",
    "val_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142e51f-512d-46b0-b358-c14e4e10727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df['text']\n",
    "y = df['stars']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d9913-c3f1-440c-a072-5a040fc8f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class balance \n",
    "y.value_counts(normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09bb81-c901-4bd0-886a-983443008c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X,y,test_size=0.25,\n",
    "                                                    random_state=321)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df6dfe-fe7e-4ec2-80e2-fb64a0627991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "y_train_full.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64dd71-77e5-46af-96ca-f12cdb4e61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = sampler.fit_resample(X_train_full.values.reshape(-1,1),y_train_full)\n",
    "X_train = X_train.flatten()\n",
    "# Check for class balance\n",
    "y_train.value_counts(normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce442700-a213-4b60-b1cb-26d7f06d48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68eef81-5633-4fa4-95a8-1a231dd9dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words in the vocabulary?\n",
    "len(count_vectorizer.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c333ca-f2c8-40bd-a7f1-cd407ef5fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform train and test data \n",
    "X_train_counts = count_vectorizer.transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2213dd-4ad4-475c-add0-c46f4c71f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83dbd5-3a65-4c8f-bfee-19d8567df5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantitate a Random Forest\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "# Obtain evaluation metrics\n",
    "evaluate_classification(clf, X_train_counts, y_train, X_test_counts, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec513dd8-d85c-48ee-bb9f-b4bdbeb2d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importances = pd.Series(clf.feature_importances_, \n",
    "                       index =  count_vectorizer.get_feature_names_out())\n",
    "importances.sort_values().tail(20).plot(kind='barh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3813210-ca6e-49af-8ebc-e0cf88291d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a vectorizer and classification model.\n",
    "clf_pipe = Pipeline([('vectorizer', CountVectorizer(stop_words='english')),\n",
    "                     ('clf',RandomForestClassifier(random_state=42))])\n",
    "clf_pipe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59575b5a-e1c3-4c67-b0c1-2667b051f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model pipeline\n",
    "clf_pipe.fit(X_train, y_train)\n",
    "# Evaluate\n",
    "evaluate_classification(clf_pipe, X_train,y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c8eea-3dcf-4e2e-8e64-cbdb3954c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importances = pd.Series(clf_pipe[-1].feature_importances_, \n",
    "                       index =  clf_pipe[0].get_feature_names_out())\n",
    "importances.sort_values().tail(20).plot(kind='barh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821117e8-9469-48f6-a36a-319992127695",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a7eff-f902-4ffb-a920-7acba8b2eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b5a3e-1ee5-4994-b04e-7deaf1524220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a vectorizer with english stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit it on the data \n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d0447-f378-4019-a595-2c6d139a1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train_vec, y_train)\n",
    "evaluate_classification(clf_nb, X_train_vec,y_train, X_test_vec, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4f464-23f5-4430-8b1d-dc3b14c9bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb.get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccca45d-2752-461d-81b1-f2f389213305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class prior probabilities\n",
    "class_priors = y_train.value_counts(normalize=True)\n",
    "class_priors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce8028-614c-46ee-85d5-32c2b22826bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define param grid\n",
    "params={'alpha': [0.001, 0.1, .5, 1, 1.5, 10, 100],\n",
    "        'fit_prior': [True, False]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76e4ed-4030-492e-93ed-d1ce65c5d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(clf_nb, param_grid=params, n_jobs = -1, verbose = 2)\n",
    "# Fit the Gridsearch on the training data\n",
    "grid_search.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836890a7-6369-4492-adb2-07d087e6f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the best combination directly\n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b330a8f-a7de-40e4-a9f7-e80dcebdb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See results of each combination of parameters\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be448925-d915-4884-b949-b937f7342d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all columns containing param_\n",
    "param_cols = cv_results.columns[ cv_results.columns.str.contains('param_')]\n",
    "# Limit results to the mean_test_score and parameters. Use the * operator to unpack the param_cols list \n",
    "results = cv_results.loc[:, ['mean_test_score', *param_cols]].sort_values(by = 'mean_test_score', ascending = False).round(6)\n",
    "results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d3276-85b4-40ce-ba1e-96b4c2270568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a vectorizer and classification model.\n",
    "clf_pipe = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                     ('clf',MultinomialNB())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e57ff-ac73-4317-b3d5-44d4e79035d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe.get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a54594-c5bf-4ec3-8c4b-f4b69b61b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"vectorizer__max_df\": [ 0.75, 0.85, 0.95],\n",
    "    'vectorizer__min_df': [ 2, 3, 4 ], \n",
    "    \"vectorizer__max_features\": [None, 500, 1000, 2000],\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2),(2,2)],\n",
    "    \"vectorizer__stop_words\": [None,'english'],\n",
    "    \"clf__alpha\": [.5, 1]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c97ea-8c2d-4bc1-92b3-410208f7ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(clf_pipe, param_grid, cv=3,verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65bcae-399c-4443-8d69-f51603a639eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86855201-a5fe-46be-afb4-9b8d17ad5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the best version of the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and Evaluate with the custom function\n",
    "evaluate_classification(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c90f2d-017f-4714-97d9-9b9e0add52d9",
   "metadata": {},
   "source": [
    "# Advanced GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd830e80-e32c-4731-a955-1d80a9493e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Pipeline to allow GridSearching different vectorizers\n",
    "vect_pipe = Pipeline([('vectorizer', CountVectorizer()), # This is just a placeholder\n",
    "                     ('clf',MultinomialNB())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d171f-43a6-4236-bfae-10e8ade16e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a param grid with options for the vectorizer\n",
    "param_grid = {\n",
    "    'vectorizer': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'clf__alpha': [.5, 1]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2dafb-a284-45de-9e5f-cbbcb82b1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search\n",
    "grid_search = GridSearchCV(vect_pipe, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d2d1a-eb25-4106-9cc4-7a017ce7eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5d321-92c2-409b-9edb-b3cfc3cdb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the best version of the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and Evaluate with the custom function\n",
    "evaluate_classification(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b7e80-585a-4fb1-90d2-a9ea75c716a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlcude a parameter shared by both vectorizers (max_df)\n",
    "param_grid = {\n",
    "    'vectorizer': [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"vectorizer__max_df\": [0.7, 0.8, 0.9],\n",
    "    'clf__alpha': [.5, 1]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f7f12-c143-4d24-8fbe-28a2788e9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params to try for both vectorizers\n",
    "param_grid_shared = {\n",
    "    \"vectorizer__max_df\": [0.7, 0.8, 0.9],\n",
    "    'vectorizer__min_df': [ 2, 3, 4 ], \n",
    "    \"vectorizer__max_features\": [None, 1000, 2000],\n",
    "    \"vectorizer__stop_words\": [None,'english']\n",
    "}\n",
    "\n",
    "# Setting params for the count vectorizer\n",
    "param_grid_count = {\n",
    "    'vectorizer':[CountVectorizer()],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "\n",
    "# Setting params for tfidf vectorizer \n",
    "param_grid_tfidf = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    \"vectorizer__norm\": [\"l1\", \"l2\"],\n",
    "    \"vectorizer__use_idf\": [True, False],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "# combine into list of params\n",
    "params_combined = [param_grid_count, param_grid_tfidf]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eee42-f3e3-450b-9a80-6cc8b5cf1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(vect_pipe, params_combined, cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73153def-4e95-4fb4-a87a-61957dcc1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963c8e8-828b-4fe4-8770-17b6d3fc825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the best version of the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and Evaluate with the custom function\n",
    "evaluate_classification(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985ed81-6d98-430c-a575-48978d32f0a9",
   "metadata": {},
   "source": [
    "## GridSearching Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c26ac7-afa1-4599-afdf-577611b92709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid with options for which vectorizer and which classifier\n",
    "param_grid = {\n",
    "    'vectorizer' : [CountVectorizer(), TfidfVectorizer()],\n",
    "    'clf' : [RandomForestClassifier(random_state = 42), MultinomialNB(), LogisticRegression(random_state = 42)]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82856e3f-0c16-4069-905d-a5d9c531c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(vect_pipe, param_grid, cv=3, scoring = 'accuracy', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86359165-6334-40cf-8d91-f923d93c2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a53785-dcab-4e54-ace9-2c2b43e767ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the best version of the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and Evaluate with the custom function\n",
    "evaluate_classification(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3f4f0-1520-4cfc-a8d6-5bffb973e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to tune multiple models\n",
    "shared_grid = {\n",
    "    'vectorizer' : [CountVectorizer(), TfidfVectorizer()]\n",
    "}\n",
    "param_grid_RF = {\n",
    "    'clf' : [RandomForestClassifier(random_state = 42)],\n",
    "    'clf__max_depth' : [5, None],\n",
    "    'clf__min_samples_leaf' : [1, 2],\n",
    "    **shared_grid\n",
    "}\n",
    "\n",
    "param_grid_MNB = {\n",
    "    'clf' : [MultinomialNB()],\n",
    "    'clf__alpha' : [.5, 1],\n",
    "    **shared_grid\n",
    "}\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'clf': [LogisticRegression(random_state = 42)],\n",
    "    'clf__C' : [.1, 1, 10, 100],\n",
    "    **shared_grid\n",
    "}\n",
    "\n",
    "# combine into list of params\n",
    "params_combined = [param_grid_RF, param_grid_MNB, param_grid_logreg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af38fbc-0270-480a-9b5c-c4ee6ae2e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(vect_pipe, params_combined, cv=3, scoring = 'accuracy', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f1ce0-27e6-48ec-9796-e0b3b5b63af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f74396-cca6-4481-a92e-09e164c23afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the best version of the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and Evaluate with the custom function\n",
    "evaluate_classification(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c20779-6800-42ef-adf6-9b9825c7ffc8",
   "metadata": {},
   "source": [
    "## Gridsearching parameters for specific vectorizers and specific models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000bf1f-9956-4a6d-a390-1196c5905dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params to try for both vectorizers\n",
    "param_grid_shared = {\n",
    "    \"vectorizer__max_df\": [0.7, 0.8, 0.9],\n",
    "    'vectorizer__min_df': [ 2, 3, 4 ], \n",
    "    \"vectorizer__max_features\": [None, 1000, 2000],\n",
    "    \"vectorizer__stop_words\": [None,'english']\n",
    "}\n",
    "\n",
    "# Setting params for the count vectorizer\n",
    "param_grid_count = {\n",
    "    'vectorizer':[CountVectorizer()],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "\n",
    "# Setting params for tfidf vectorizer \n",
    "param_grid_tfidf = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    \"vectorizer__norm\": [\"l1\", \"l2\"],\n",
    "    \"vectorizer__use_idf\": [True, False],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "# combine into list of params\n",
    "vect_params_combined = [param_grid_count, param_grid_tfidf]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70af0a-7ca6-49be-aa54-12a1d162d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to tune multiple models\n",
    "\n",
    "param_grid_RF = {\n",
    "    'clf' : [RandomForestClassifier(random_state = 42)],\n",
    "    'clf__max_depth' : [5, None],\n",
    "    'clf__min_samples_leaf' : [1, 2],\n",
    "}\n",
    "\n",
    "param_grid_MNB = {\n",
    "    'clf' : [MultinomialNB()],\n",
    "    'clf__alpha' : [.5, 1],\n",
    "}\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'clf': [LogisticRegression(random_state = 42)],\n",
    "    'clf__C' : [.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# combine into list of params\n",
    "model_params_combined = [param_grid_RF, param_grid_MNB, param_grid_logreg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a208af-6bde-4e6f-8ee3-483f222ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "paired_param_grids = list(itertools.product(vect_params_combined, model_params_combined))\n",
    "paired_param_grids[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91076769-b494-4b32-a397-dcaab32112c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = []\n",
    "for vector_params, model_params in  paired_param_grids:\n",
    "    combined = {**vector_params, **model_params}\n",
    "    final_params.append(combined)\n",
    "final_params[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6a428-d0c1-427d-9d15-47e27ee83c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(vect_pipe, final_params, cv=3, scoring = 'accuracy', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23671a-f343-4880-ad80-14f421328d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40db16-9d4b-46bf-a057-737737dd11cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e8b7f-1e1d-45db-91cd-ae222b4bb08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a852af-f2b2-441c-a36a-58444ac7049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd6d60-f0a7-4c20-aafd-b54021dbb85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec05c3-7eb1-4328-90dc-eb407f31d922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
