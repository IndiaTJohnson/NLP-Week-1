{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd14ba6-8884-40df-bccf-4f1157424d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .lower()\n",
    "text = \"Hello, World!\"\n",
    "lower_text = text.lower()\n",
    "lower_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec504186-f984-467d-ab31-753e69b8f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .upper()\n",
    "text = \"Hello, World!\"\n",
    "upper_text = text.upper()\n",
    "upper_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667f53a-bc0d-4977-b8cf-31742fbc6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .strip() to remove whitespace\n",
    "text = \"  extra spaces  \"\n",
    "stripped_text = text.strip()\n",
    "stripped_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97474e36-77c8-4f40-99a3-4722e3202951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .strip() to remove a string\n",
    "text = \"  extra spaces!?!?\"\n",
    "stripped_text = text.strip(\"!?!?\")\n",
    "stripped_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186e244-89b4-4b00-8919-51c94224e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .split() on whitespaces\n",
    "text = \"apple, orange, and banana\"\n",
    "split_text = text.split()\n",
    "split_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d90c9-9cea-4070-abcc-3b549c8e7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .split() on whitespaces\n",
    "text = \"apple, orange, and banana\"\n",
    "split_text = text.split(\",\")\n",
    "split_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfd596-b90d-4e78-aba3-9777b007e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .join()\n",
    "words = ['apple', 'orange', 'banana']\n",
    "joined_text = \", \".join(words)\n",
    "joined_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd46ae-062b-4d76-a9c7-d1cccec11e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .replace()\n",
    "text = \"I like apples.\"\n",
    "new_text = text.replace(\"apples\", \"oranges\")\n",
    "new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d5192-9438-47c9-b843-81346c6d5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .startswith()\n",
    "text = \"Hello, World!\"\n",
    "starts_with = text.startswith(\"Hello\")\n",
    "starts_with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ad5c5-451c-44e0-b1d8-03fb861acb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example .endswith()\n",
    "text = \"Hello, World!\"\n",
    "ends_with = text.endswith(\"World!\")\n",
    "ends_with\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047ef0f-7134-45d4-8440-181ef776a04f",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac192ec6-cbc8-45cb-af15-9a4498ac4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0c7c1-773d-4928-9b23-33aec02e5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Contact us at support@example.com and sales@example2.com for more info.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504969d-8a42-42d8-b9dc-3f6b7b203b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = 'us'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95fb9a-71ab-4913-bb9e-7b82677198f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'[aeiou]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a8f03-ea84-4d27-bc7d-a7c7d7b41684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'[c-f]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8ee94-b15f-483d-aad8-10006e76c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6096599-3bcd-4a17-8d67-6f4ee5b4b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98da98-eea0-4c0f-b917-e09527f303ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2b640-a0c1-4cf6-8811-db2e373b5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a49d83-e73b-4b8b-ad26-3b6f8fdbb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z]+@'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4694141-c1ae-4ca8-ac3b-aeac76e8bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b18fa7-452d-44ae-bf76-9c69cb46c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with improper email and non-email \"@\"\n",
    "text = \"Contact us at support@example.com and sales@example2.com and help@tech for more info. We look forward to seeing you@5pm!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c02dd5-54ce-44e2-a792-a6588ddf5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603f593-674d-41d5-be46-74f67ec1dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe1fea-0d33-46f6-9250-22e5d57c390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, text)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d74066-51dc-4502-9bb9-70c51a66ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid and invalid emails\n",
    "email_test = 'ninja@codingdojo.com, ninjacodingdojo.com, ninja@codingdojo, ninja@codingdojo.y'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682bbe0-0c19-428d-8fe6-928bc22cc9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]+'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, email_test)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087264d-3671-480a-882f-04c3bf1ca205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern - FINAL Pattern for Identifying emails\n",
    "pattern = r'\\b[A-Za-z._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}'\n",
    "# Use re.findall() to locate all instances of the pattern\n",
    "result =  re.findall(pattern, email_test)\n",
    "# Print the result\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966f523-cf38-47af-82c4-236fba26f655",
   "metadata": {},
   "source": [
    "## Summary\r\n",
    "In this lesson, we introduced regular expressions by demonstrating the building and testing of a pattern to identify emails. Regex patterns can look quite complex at first glance, but breaking them down into smaller pieces can help you recognize what each element in the pattern is achieving. Developing the pattern often requires an iterative process with testing. As we demonstrated, you should consider including edge cases and non-examples in your test text to ensure the pattern will work as expected.\r\n",
    "\r\n",
    "\n",
    "\n",
    "Resources\r\n",
    "www.regexr.com\r\n",
    "\r\n",
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d275be-fdd5-4aa9-b1d3-9777d930fd2f",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772f3a5-fa5d-446a-826d-e7a1e4382bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax\n",
    "with open(fpath) as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fa732-3629-48e3-8627-7a4a445a7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Windows\n",
    "with open(fpath, encoding = \"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd0803-92b4-4f74-a2a4-007b7ed91d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Mac\n",
    "with open(fpath, encoding = \"cp1252\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa600d55-daf6-4343-9441-c9a525c831a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1,000 characters (printing)\n",
    "print(txt[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79c3c6-2019-4119-90bd-2570bf934253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b5b8a-394b-4b29-9cd3-89b971d2442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(random_state = 123).generate(txt)\n",
    "plt.imshow(cloud);\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c6e23-fdac-4836-a74c-c97b834fb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "STOPWORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79beaac-5c91-462b-b9c3-9ca6121a2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom stopwords\n",
    "custom_stopwords = [\"said\",'Alice','s', *STOPWORDS]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c84fc6-1851-4386-b3af-0e330d296c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(random_state=123, stopwords=custom_stopwords).generate(txt)\n",
    "plt.imshow(cloud);\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a46e2e-8800-4ea3-bd86-72dbc0190a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different color map\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    colormap=\"plasma\"\n",
    ").generate(txt)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63573097-60f2-49db-804c-edfd5e47eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"plasma\",\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c8272-1b70-4f05-be46-ec1571774b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"plasma\",\n",
    "    width = 800,\n",
    "    height = 400\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02878749-60e1-4979-b1db-c777e0445e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a WordCloud and use the generate method\n",
    "cloud = WordCloud(\n",
    "    random_state=123,\n",
    "    stopwords = custom_stopwords,\n",
    "    background_color=\"white\",\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "    max_words=200,\n",
    "    colormap=\"plasma\",\n",
    "    min_word_length=2,\n",
    ").generate(txt)\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbdf81-777a-45d7-a04a-b54ffa2e1ef7",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc785-0195-479b-bc11-7bca6fb6d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text for exploration\n",
    "sample_text = \"Hey there, Ninjas! I'm so excited to tell you about NLP. It's super-cool, isn't it? I've been #learning a lot. Follow @NLP for updates. E-mail me at fake_address@email.com\"\n",
    "sample_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a67e4-3900-4173-bfdd-cd0cb61543c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect lowercase text\n",
    "sample_text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f86dff-03ff-457c-babb-5fab0665ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace Tokenization\n",
    "whitespace_tokens = sample_text.lower().split()\n",
    "# Print tokens\n",
    "print('Whitespace tokens: \\n',whitespace_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39777146-f67c-4c41-b7fa-7c51469e177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To more easily see each token, display instead of print\n",
    "whitespace_tokens[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9406e-9b10-4cc3-bb91-6a7861bcb4d6",
   "metadata": {},
   "source": [
    "One very common NLP library is NLTK (Natural Language Toolkit).\r\n",
    "## \r\n",
    "Important Note: NLTK Downloads\r\n",
    "\r\n",
    "​You will encounter error messages the first time you try to run several tools from NLTK. The error message has the exact commands you need to run to download the required component. \r\n",
    "\r\n",
    " It will use the  \"nltk.download()\" function, and the error message tells you what to use as the argument for nltk.download. \r\n",
    "\r\n",
    "e.g. \"nltk.download('punkt')\"\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb9e4b-14ce-4aff-8259-73a64848ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# NLTK's Word Tokenization\n",
    "word_tokens = word_tokenize(sample_text.lower())\n",
    "print(\"Original text: \\n\", sample_text, '\\n\\n')\n",
    "print('Word tokens: \\n', word_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88c4cf-5c7c-45b5-b064-a19f7190bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# NLTK's Tweet Tokenization\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sample_text.lower())\n",
    "print(\"Original text: \\n\", sample_text, '\\n\\n')\n",
    "print(f\"Tweet Tokens: \\n\", tweet_tokens,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f64f5-ab6b-4e14-a2ff-85c14f7e73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "# Define bigrams\n",
    "bigrams = ngrams(tweet_tokens, 2)\n",
    "# display bigrams\n",
    "list(bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a6096-f32e-40e1-b93a-07b3051aada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trigrams\n",
    "trigrams = ngrams(tweet_tokens, 3)\n",
    "# display trigrams\n",
    "list(trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fff929-97fc-4280-88c3-2ba90888183b",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dd166-d35c-453c-b13c-a030f94289e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windows\n",
    "# Define the filepath\n",
    "fpath = 'Data/alice_in_wonderland.txt'\n",
    "# Use with Open syntax\n",
    "with open(fpath, encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "print(f\"there are {len(txt)} characters in the full text.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe1da3-4706-4fa6-8810-cc58eb7c6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mac\n",
    "# Define the filepath\n",
    "fpath= \"Data/Alice_in_Wonderland.txt\"\n",
    "# Use with open syntax--avoid error on Mac\n",
    "with open(fpath, encoding = \"cp1252\") as f:\n",
    "    txt = f.read()\n",
    "# Report length of the text\n",
    "print(f\"There are {len(txt)} characters in the full text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0579cc-b790-46eb-ac5d-47b506488622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = txt.lower().split()\n",
    "tokens[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121bcc5-e825-436a-8014-2ba8f6587cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06364490-0aa2-4b69-b165-9074e9c1d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd115a-636f-4cbd-9d73-f94ddcea5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.plot(20, title='Frequency of Words in Alices in Wonderland')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe45a62-5526-4212-89ff-009196dd4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.plot(20, title='Frequency of Words in Alices in Wonderland', percents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855313b-4baf-45d9-a6c6-21b76cb37a3f",
   "metadata": {},
   "source": [
    "Finally, we can use matplotlib to manipulate these plots, but ONLY if we set 'show=False' when calling the .plot() method. In this case, the .plot() method will return a matplotlib ax object. This would be useful, for example, if you wanted to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569988f-941f-4b5e-9942-2cc3f577e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = dist.plot(20, show=False)\n",
    "ax.set_title('Number of Occurances of Top 20 Words')\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('frequency_distribution.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c7356-4ed4-48fe-b694-c2a1a9989122",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b39bd-af8f-45bb-bf5e-69dfe0bf002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Example text\n",
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "words =  sample_text.split(' ')\n",
    "print(\"Original Words:\\n\", words,'\\n\\n')\n",
    "# Applying stemming\n",
    "stemmed_words = [stemmer.stem(w.lower()) for w in words]\n",
    "print(\"Stemmed Words:\\n\", stemmed_words,'\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e32d6-8883-48f4-8e01-2eea2990f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Original Words:\\n\", words,'\\n')\n",
    "# Lemmatizing example text\n",
    "lemmas = [lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "print(\"Lemmas:\\n\", lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0653e-10e5-4ed8-94da-55cec1676688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing 'running' as a verb\n",
    "print(\"As a verb:\", lemmatizer.lemmatize('running', pos='v'))\n",
    "# Lemmatizing 'running' as a noun\n",
    "print(\"As a noun:\", lemmatizer.lemmatize('running', pos='n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd9b8b-3608-40b9-9777-9919c1386057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing 'was' as default(noun)\n",
    "print(\"As default(noun):\", lemmatizer.lemmatize('was'))\n",
    "# Lemmatizing 'was' as a verb\n",
    "print(\"As verb:\", lemmatizer.lemmatize('was', pos='v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963c201-88a5-40f6-b3d3-d3ab34536117",
   "metadata": {},
   "source": [
    "## NLP with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a56-aaec-49ee-b334-1b555a63b70f",
   "metadata": {},
   "source": [
    "Official 101 to SpaCy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Installing SpaCy\r\n",
    "\r\n",
    "Use spaCy's usage page and enter your OS/system info to get the installation commands for your computer.\r\n",
    "We want:\r\n",
    "Package Manager: Conda\r\n",
    "Hardware: CPU\r\n",
    "Trained Pipelines: English\r\n",
    "Select Model for: Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bd879-9dd6-4ed8-b4f1-9ae5b81f2942",
   "metadata": {},
   "source": [
    "(In your terminal)\r\n",
    "\r\n",
    "conda install -c conda-forge spacy\r\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc883490-b1f6-41c5-9563-7ad191af9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741f775-2a04-4862-af35-0b417d6fa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in the pipeline\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbca72a-f2c5-4487-b86d-ec876ce2edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable ner\n",
    "nlp_no_ner = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "# Print active components\n",
    "nlp_no_ner.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeabc94-7be9-446c-a358-266d39e7fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text for demonstration\n",
    "sample_text = \"While running in Central Park, \\nI noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\"\n",
    "print(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618138fc-f18f-4374-86bd-7f169d42037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc with the nlp pipeline\n",
    "doc = nlp(sample_text)\n",
    "type(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335dbc8-2a2b-428d-acf2-d4ed86a40ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bec12-9d5c-43ec-8364-34dd26602f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 10 tokens separately\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ff1c0-dac7-404e-9e33-065fbe87c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a token from the doc\n",
    "token = doc[1]\n",
    "token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969d91a-ec75-44b1-b5a4-f3e161f14646",
   "metadata": {},
   "source": [
    "a) token.text: The original form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c4ed9-d741-47fb-9f98-acc128c47ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662bbae-973e-41a3-bc19-9ecf781de1cb",
   "metadata": {},
   "source": [
    "b) token.lemma_: The base or root form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0d0e4-5402-4d1b-877a-bb4b4bc2da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e88f0-d5d3-47d0-9da8-70f7f08880d6",
   "metadata": {},
   "source": [
    "c) token.pos_: The part-of-speech tag associated with the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d843df1-6a49-4a96-ad32-0523ef16e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.pos_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1043e5-b11b-4c0a-bbff-1303085d295a",
   "metadata": {},
   "source": [
    "d) token.is_stop: Boolean flag to check if the token is a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca368bc3-ebfe-4167-b01d-b59a62efe3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88554d5-be68-43ca-a99c-57c562c0e2ba",
   "metadata": {},
   "source": [
    "e) token.is_punct: Boolean flag to check if the token is punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd230c-a92c-4e02-863a-9e3caedf7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_punct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a349f84-a0f4-43f9-8dc1-494bb457233c",
   "metadata": {},
   "source": [
    "f) token.is_space: Boolean flag to check if the token is a whitespace character (.e.g new line \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c72c53-bf3e-43a5-b610-dc432a377957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.is_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63ae63-aad8-47bb-a77a-660eccf7a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create dictionary for desired attributes for each token\n",
    "token_data = []\n",
    "for token in doc:\n",
    "    token_dict = {\n",
    "        \".text\": token.text,\n",
    "        \".lemma_\": token.lemma_,\n",
    "        \".pos_\": token.pos_,\n",
    "        \".is_stop\": token.is_stop,\n",
    "        \".is_punct\": token.is_punct,\n",
    "        \".is_space\": token.is_space\n",
    "    }\n",
    "    token_data.append(token_dict)\n",
    "# Save dictionary as a dataframe\n",
    "spacy_df = pd.DataFrame(token_data) \n",
    "spacy_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076179b1-f0be-4f1f-af61-b52e4ca4dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to remove stopwords\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword, skip it\n",
    "    if token.is_stop == True:\n",
    "        continue \n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "print(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81580578-3736-4662-877a-9c18b2f93db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    ##NEW: \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "        \n",
    "print(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20e35b-e577-4b99-8dca-a81b52f9cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_lemmas = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # # keep the tokens'.text for the final list of tokens\n",
    "        # cleaned_tokens.append(token.text.lower())\n",
    "        # keep the tokens's .lemma_ for the final list of tokens\n",
    "        cleaned_lemmas.append(token.lemma_.lower())\n",
    "        \n",
    "print(cleaned_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7149549-4741-4243-a475-55e061fba8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text and lemmas\n",
    "print(\"Tokenized words:\\n\", cleaned_tokens,\"\\n\")\n",
    "print(\"Lemmatized words:\\n\", cleaned_lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b7afe-e80a-4a9e-98b5-18dc69ee52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc, remove_stopwords=True, remove_punct=True, use_lemmas=False):\n",
    "    \"\"\"Temporary Fucntion - for Education Purposes (we will make something better below)\n",
    "    \"\"\"\n",
    "    tokens = [ ]\n",
    "    for token in doc:\n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_stopwords == True) and (token.is_stop == True):\n",
    "            # Continue the loop with the next token\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_punct == True):\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_space == True):\n",
    "            continue\n",
    "    \n",
    "        ## Determine final form of output list of tokens/lemmas\n",
    "        if use_lemmas:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e7e3b-5372-4fd9-9b4b-a1019857afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to a doc.\n",
    "doc = nlp(sample_text)\n",
    "# Tokenizing, keeping stopwords and punctuatin\n",
    "dirty_tokens = preprocess_doc(doc, remove_stopwords=False,remove_punct=False)\n",
    "print(dirty_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef2396-527a-4cf7-8a4d-12b22878b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, removing stopwords and punctuation\n",
    "cleaned_tokens = preprocess_doc(doc, remove_stopwords=True,remove_punct=True)\n",
    "print(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67558f-ce5c-4e57-8c00-1ad7da70a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing, removing stopwords and punctuation\n",
    "cleaned_lemmas = preprocess_doc(doc, remove_stopwords=True,remove_punct=True, use_lemmas=True)\n",
    "print(cleaned_lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d02343-a5bb-402e-8457-a8dabad89ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Example Framework (Not runnable)\n",
    "lists_of_texts = [text1, text2, text3]\n",
    "processed_texts = []\n",
    "for doc in nlp.pipe(list_of_texts):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        # ... the same logic from our preprocess docs function.\n",
    "        doc_tokens.append(token.text.lower())\n",
    "        \n",
    "    # Append the list of tokens for current doc to processed_texts\n",
    "    processed_texts.append(doc_tokens)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0aab4-2ab9-4243-8c5e-e52123dcf1cc",
   "metadata": {},
   "source": [
    "# Be sure to save this function. You will add it to your custom_functions.py file in an upcoming lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd04304-7d76-4d80-97da-add3aaa4ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    processed_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44dc11-53ed-410d-bc71-6bc478b1a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default args will produce tokens\n",
    "tokens = batch_preprocess_texts([sample_text])\n",
    "tokens = tokens[0]\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b0df2-2173-43d0-bace-08be9a6b83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting use_lemmas = True will produce lemmas\n",
    "lemmas = batch_preprocess_texts([sample_text], use_lemmas=True)\n",
    "lemmas = lemmas[0]\n",
    "print(lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8978026-a5dd-4ae5-b849-cc5e22331e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "doc = nlp(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73023316-06bd-4c50-be83-4eec756a2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from doc\n",
    "sentences = list(doc.sents)\n",
    "len(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e443-4632-45e3-a28d-ef1e0ffb30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first sentence\n",
    "sentences[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6688f1-e4a5-4432-ac89-4afdfef619ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print any named entities in the doc and its label\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04aa494-0323-4271-a682-a9bc09d663de",
   "metadata": {},
   "source": [
    "# Text in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d755dc-71aa-4846-af90-b222d836d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4d51d-5543-42a1-9932-719a0be03c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "​def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deacead-21c9-402a-9d44-763f77894576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_businesses = pd.read_csv(\"Data/yelp-business-metadata.csv.gz\", index_col='business_id')\n",
    "df_businesses.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac8411-1ad4-4703-bc96-16c82748209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the reviews for CA 2015-2018\n",
    "df = pd.read_csv('Data/yelp-reviews-CA-2015-2018.csv.gz', index_col='review_id')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255c2a0-b6e6-4037-b694-1b90397a7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New pandas option to change: \n",
    "pd.get_option('display.max_colwidth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbebe3-a11c-42c0-9e0d-35f610742ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase column width\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb049b-9233-4a1c-ad78-36cfabafaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using city\n",
    "filter_city = df_businesses['city'].str.contains('Santa Barbara') \n",
    "filter_city.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc62dc-4bf0-4742-a65c-2fd3421e7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using name\n",
    "filter_name =  df_businesses['name'].str.contains(\"Sandbar\")\n",
    "filter_name.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b9217-6e56-4067-898a-ec452190cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the target business\n",
    "selected_business =df_businesses.loc[ filter_name & filter_city]\n",
    "selected_business\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bcf7a-d458-4df5-9d41-2d7bff0dbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the business id for slicing the reviews\n",
    "business_id = selected_business.index[0]\n",
    "business_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e547763-f349-4b9a-8fde-9ec1ec8f74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep reviews for the selected business\n",
    "reviews = df.loc[ df['business_id']==business_id]\n",
    "reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd343b5f-a238-4fba-9518-dbc4a52fbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample review\n",
    "sample_review = reviews.iloc[0]\n",
    "sample_review['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e6a5c-311d-439c-a8d1-16e8c7730434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each star rating?\n",
    "sns.countplot(data = reviews, x = 'stars');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c98b7c-798b-4397-84ee-d06d74c7b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to only 1 and 5 star reviews\n",
    "reviews = reviews[reviews['stars'].isin([1,5])]\n",
    "reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe94dfc-7609-4334-a68e-0a837920d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What languages are represented?\n",
    "reviews['language'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b8655-71a3-4133-a558-3f7fa9f86ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data to only English\n",
    "reviews = reviews[reviews['language']=='en']\n",
    "reviews['language'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d0a2f-0d9e-479e-92ea-ae071d76d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many 1 and 5 star reviews?\n",
    "reviews['stars'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f831-b489-426d-b242-0b81577911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop business_id and language\n",
    "reviews = reviews.drop(columns=['business_id', 'language'])\n",
    "# Make data a datetime object\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f30e4-a24a-4ffb-b5f2-c4439f99be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Disable parser and ner\n",
    "nlp_light = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])\n",
    "# Print active components\n",
    "nlp_light.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3413f1-3ed1-491f-ad28-61c42c919ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch preprocess the text and store tokens\n",
    "reviews['tokens'] = batch_preprocess_texts(reviews['text'], nlp = nlp_light)\n",
    "reviews.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb5af7-446a-4fb4-bce5-4c37cbbb78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch preprocess the text and store lemmas\n",
    "reviews['lemmas'] = batch_preprocess_texts(reviews['text'], nlp = nlp_light, use_lemmas = True)\n",
    "reviews.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77a428-dd03-48e8-ac02-9fe5df871d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of tokens\n",
    "sample_review = reviews.iloc[0]\n",
    "sample_review['tokens']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdab23-8103-4199-a7dc-4ffd66028c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data type of tokens\n",
    "type(sample_review['tokens'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ca54c-dee2-42ef-9ca2-b82ca8ecaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data type of lemmas\n",
    "type(sample_review['lemmas'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c82bc8-1e9d-4633-9207-49f8209976c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join list of tokens into a string with spaces between each token\n",
    "reviews['tokens-joined'] = reviews['tokens'].map(lambda x: \" \".join(x))\n",
    "# Join list of lemmas into a string with spaces between each lemma\n",
    "reviews['lemmas-joined'] = reviews['lemmas'].map(lambda x: \" \".join(x))\n",
    "reviews.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130acafc-368c-440b-a5e4-a7def5e2e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first review as sample\n",
    "sample_review = reviews.iloc[0]\n",
    "# confirm data type of tokens-joined\n",
    "print(type(sample_review['tokens-joined']))\n",
    "# confirm data type of lemmas-joined\n",
    "print(type(sample_review['lemmas-joined']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7d351-0ca0-40cd-807e-245c29f07407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the tokens-joined into a single string\n",
    "sample_review['tokens-joined']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a4a4d-a44f-408e-851a-37ea19ad5bed",
   "metadata": {},
   "source": [
    "## Comparing Groups - Word Clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8feca-0a9f-4879-ad80-e39aafbfdc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset from previous lesson\n",
    "reviews.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc3a73-a86c-414b-9073-f31ead25e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters for 1 and 5 star reviews\n",
    "filter_high = reviews['stars'] == 5\n",
    "filter_low = reviews['stars'] == 1\n",
    "filter_high.sum(), filter_low.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4f5ee-db24-4b96-9eee-a2bfece290e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4448d-cb6a-4f48-a3fe-d47e0f9bf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star reviews\n",
    "high_reviews_text = \" \".join( reviews.loc[filter_high, 'text'])\n",
    "print(high_reviews_text[:1000],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5a17e-764a-4bbe-a79d-7175cde31523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 1 star reviews\n",
    "low_reviews_text = \" \".join( reviews.loc[filter_low, 'text'])\n",
    "print(low_reviews_text[:1000],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa875f-5983-4c87-ac46-9c65d1e1aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of raw text\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_text)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_text)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Raw Reviews', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d16806-cfe1-4722-b864-3220284dac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star tokens\n",
    "high_reviews_tokens = \" \".join( reviews.loc[filter_high, 'tokens-joined'])\n",
    "# Make a single giant string with entire group of 1 star tokens\n",
    "low_reviews_tokens = \" \".join( reviews.loc[filter_low, 'tokens-joined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ca36e-ea38-411b-87c5-5ac3d8776e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of processed tokens\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_tokens)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_tokens)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Tokens', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3962fb-322e-44c8-8f23-af193cfa9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single giant string with entire group of 5 star lemmas\n",
    "high_reviews_lemmas = \" \".join( reviews.loc[filter_high, 'lemmas-joined'])\n",
    "# Make a single giant string with entire group of 1 star lemmas\n",
    "low_reviews_lemmas= \" \".join( reviews.loc[filter_low, 'lemmas-joined'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225a0cb-d6b9-4322-96fe-b224cb353269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of lemmas\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                     ).generate(low_reviews_lemmas)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                      ).generate(high_reviews_lemmas)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Lemmas', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25bbc6-975c-4860-9c19-96964601ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "custom_stopwords = ['good', 'great', 'Sandbar', 'Santa', 'Barbara', 'place', 'come', 'drink']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581019a6-6be6-4d7c-ac6b-95d5d24c5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word clouds of lemmas with custom stopwords\n",
    "# Create an instance of a WordCloud and use the generate method\n",
    "low_cloud = WordCloud(random_state = 42,\n",
    "                      width = 800,\n",
    "                      height = 1000,\n",
    "                      min_word_length = 2, colormap=\"Reds\",\n",
    "                      stopwords = custom_stopwords\n",
    "                     ).generate(low_reviews_lemmas)\n",
    "high_cloud = WordCloud(random_state = 42,\n",
    "                       width = 800,\n",
    "                       height= 1000,\n",
    "                       min_word_length = 2,\n",
    "                       colormap=\"Blues\",\n",
    "                       stopwords = custom_stopwords\n",
    "                      ).generate(high_reviews_lemmas)\n",
    "# Plot the wordclouds side by side\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "axes[0].imshow(low_cloud, interpolation='bilinear')\n",
    "axes[0].set_title(\"Low Ratings\")\n",
    "axes[1].imshow(high_cloud, interpolation='bilinear')\n",
    "axes[1].set_title(\"High Ratings\")\n",
    "[ax.axis('off') for ax in axes]\n",
    "fig.tight_layout();\n",
    "fig.suptitle('Word Clouds - Lemmas', fontsize=20, y=1.05);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239fb89-e3f9-4c9c-9405-d805fda8ca74",
   "metadata": {},
   "source": [
    "## Comparing Groups: Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf118c19-bab7-4a49-8901-3d3855e53360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Freqdist\n",
    "from nltk.probability import FreqDist\n",
    "# Split the lemmas into individual token words\n",
    "low_review_lemmas_split = low_reviews_lemmas.split()\n",
    "# Pass the tokenized lemmas to the class constructor and plot the distribution \n",
    "low_dist = FreqDist(low_review_lemmas_split)\n",
    "ax = low_dist.plot(20, show = False, title='Distribution of Words in One-Star Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('low_review_freq_dist.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e9efc-b4ea-47a3-b9cb-514b9302af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lemmas into individual token words\n",
    "high_review_lemmas_split = high_reviews_lemmas.split()\n",
    "# Pass the tokenized lemmas to the class constructor and plot the distribution \n",
    "low_dist = FreqDist(high_review_lemmas_split)\n",
    "ax = low_dist.plot(20, show=False, title='Distribution of Words in Five-Star Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('high_review_freq_dist.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ad33b-b191-427e-a97a-7775d61caf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lemmas in high reviews\n",
    "exploded_high_review_lemmas = reviews.loc[filter_high, 'lemmas'].explode()\n",
    "exploded_high_review_lemmas.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495e32c-1d49-4c4b-ab87-3ebc41788e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lemmas in low reviews\n",
    "exploded_low_review_lemmas = reviews.loc[filter_low, 'lemmas'].explode()\n",
    "exploded_low_review_lemmas.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5528a8-0015-4e62-a6b7-efc4678a3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of all lemmas in both high and low review groups\n",
    "high_review_lemmas_list = reviews.loc[filter_high, 'lemmas'].explode().to_list()\n",
    "low_review_lemmas_list = reviews.loc[filter_low, 'lemmas'].explode().to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d4d0a-dc67-43e1-9e4a-d7edab0fd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of words in high reviews\n",
    "high_dist = FreqDist(high_review_lemmas_list)\n",
    "high_dist.plot(20, title='Distribution of Words in 5 Star Reviews')\n",
    "# Plot distribution of words in low reviews\n",
    "low_dist = FreqDist(low_review_lemmas_list)\n",
    "low_dist.plot(20, title='Distribution of Words in 1 Star Reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac561c-cf2f-463a-887f-4b0fd6ab2630",
   "metadata": {},
   "source": [
    "## Comping Groups: N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96704c-27ce-4681-909c-4d84a11e08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1000 tokens in high reviews\n",
    "high_reviews_tokens[: 1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6baa0-40cd-43c6-83de-b204440b0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 1000 tokens in low reviews\n",
    "low_reviews_tokens[: 1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895863b1-6090-4445-8b4d-6b54f93016e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split single string into individual list elements\n",
    "high_reviews_tokens_split = high_reviews_tokens.split()\n",
    "high_reviews_tokens_split[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc700216-a0a8-48d9-be83-9eae3df88546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split single string into individual list elements\n",
    "low_reviews_tokens_split = low_reviews_tokens.split()\n",
    "low_reviews_tokens_split[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fd577-7a2c-4898-b049-c0cd283a9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview our dataframe\n",
    "reviews.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1393d6-2e15-406f-87e0-24b014cba7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain single list of tokens from all 5 star reviews\n",
    "high_reviews_tokens_exploded_list = reviews.loc[filter_high, 'tokens'].explode().to_list()\n",
    "high_reviews_tokens_exploded_list[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09102359-9fc3-401e-a0b0-888398a39348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain single list of tokens from all 1 star reviews\n",
    "low_reviews_tokens_exploded_list = reviews.loc[filter_low, 'tokens'].explode().to_list()\n",
    "low_reviews_tokens_exploded_list[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12766079-8c42-4db5-85b7-f1cebf55647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Instantiate a measures objects for Bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3d9dd-bb89-4e4c-b16a-701e2a6f03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a finder for the high reviews\n",
    "bigram_finder_high = nltk.BigramCollocationFinder.from_words(high_reviews_tokens_exploded_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350d3d1-d987-4bdd-aa57-db375e580b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scores for the bigrams using the score_ngrams method \n",
    "# Include the desired scoring method of the measures object\n",
    "bigrams_scores_high = bigram_finder_high.score_ngrams(bigram_measures.raw_freq)\n",
    "bigrams_scores_high[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b6814-2002-47cc-9e0f-8613bbaca2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a finder for the low reviews\n",
    "bigram_finder_low = nltk.BigramCollocationFinder.from_words(low_reviews_tokens_exploded_list)\n",
    "\n",
    "# Obtain scores for the bigrams using the score_ngrams method \n",
    "# Include the desired scoring method of the measures object\n",
    "bigrams_scores_low = bigram_finder_low.score_ngrams(bigram_measures.raw_freq)\n",
    "bigrams_scores_low[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eeb602-55f9-47b9-856d-587cc84ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to dataframe\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_scores_high, columns=['Words','Frequency'])\n",
    "df_bigram_scores_high.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb55b0b-349c-4d79-b81c-7e0c19216a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of tuples to dataframe\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_scores_low, columns=['Words','Frequency'])\n",
    "df_bigram_scores_low.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83621f2d-4abe-41c1-a47a-b5323dc4fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine low and high reviews score dfs and add a group name as multi-index\n",
    "df_compare_bigrams = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ff26-70dc-42ef-9ccb-f68f91a70465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the pmi score with the score_ngrams\n",
    "bigrams_pmi_high = bigram_finder_high.score_ngrams(bigram_measures.pmi)\n",
    "# Repeat for low reviews\n",
    "bigrams_pmi_low = bigram_finder_low.score_ngrams(bigram_measures.pmi)\n",
    "# Preview results\n",
    "bigrams_pmi_high[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331da81-9219-4d5c-9b73-c1aef4956387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for high review pmi scores\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_pmi_high, columns=['Words','PMI Score'])\n",
    "\n",
    "# df for low review pmi scores\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_pmi_low, columns=['Words','PMI Score'])\n",
    "\n",
    "\n",
    "# Combine both groups and add a group name as multi-index\n",
    "df_compare_bigrams_pmi = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams_pmi.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4903c18-02ad-420c-a82d-75bd8dbcc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired minimum frequency value\n",
    "min_frequency = 3 \n",
    "# Apply filter to finder for the high reviews\n",
    "bigram_finder_high.apply_freq_filter(min_frequency)\n",
    "# Apply filter to finder for the low reviews\n",
    "bigram_finder_low.apply_freq_filter(min_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cc29e-01a3-4506-b8cb-10b338f6a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the pmi score with the score_ngrams\n",
    "bigrams_pmi_high = bigram_finder_high.score_ngrams(bigram_measures.pmi)\n",
    "# Repeat for low reviews\n",
    "bigrams_pmi_low = bigram_finder_low.score_ngrams(bigram_measures.pmi)\n",
    "# Preview results\n",
    "bigrams_pmi_high[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad07f8f-4237-441e-9b60-0adef2d62541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for high review pmi scores\n",
    "df_bigram_scores_high = pd.DataFrame(bigrams_pmi_high, columns=['Words','PMI Score'])\n",
    "\n",
    "# df for low review pmi scores\n",
    "df_bigram_scores_low = pd.DataFrame(bigrams_pmi_low, columns=['Words','PMI Score'])\n",
    "\n",
    "\n",
    "# Combine both groups and add a group name as multi-index\n",
    "df_compare_bigrams_pmi = pd.concat(\n",
    "    [df_bigram_scores_high, df_bigram_scores_low],\n",
    "    axis=1,\n",
    "    keys=[\"High Rating Reviews\", \"Low Rating Reviews\"],\n",
    ")\n",
    "df_compare_bigrams_pmi.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9b2ca-17ad-4011-b206-e3f48a565aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075db07-af98-4853-b51f-d5302f5cb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tri-grams\n",
    "nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# For quad-grams\n",
    "nltk.collocations.QuadgramAssocMeasures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac06d2-bc7f-4527-ac71-a956f0f01b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.collocations.BigramCollocationFinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd8cdd-a939-4f34-a2f8-e763f98297b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tri-grams\n",
    "nltk.collocations.TrigramCollocationFinder\n",
    "\n",
    "# For quad-grams\n",
    "nltk.collocations.QuadgramCollocationFinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b7679-4247-4795-a46a-bde2c2ed5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_ngram_measures_finder(tokens, ngrams=2, measure='raw_freq', top_n=None, min_freq = 1,\n",
    "                             words_colname='Words'):\n",
    "    import nltk\n",
    "    if ngrams == 4:\n",
    "        MeasuresClass = nltk.collocations.QuadgramAssocMeasures\n",
    "        FinderClass = nltk.collocations.QuadgramCollocationFinder\n",
    "        \n",
    "    elif ngrams == 3: \n",
    "        MeasuresClass = nltk.collocations.TrigramAssocMeasures\n",
    "        FinderClass = nltk.collocations.TrigramCollocationFinder\n",
    "    else:\n",
    "        MeasuresClass = nltk.collocations.BigramAssocMeasures\n",
    "        FinderClass = nltk.collocations.BigramCollocationFinder\n",
    "\n",
    "    measures = MeasuresClass()\n",
    "    \n",
    "   \n",
    "    finder = FinderClass.from_words(tokens)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    if measure=='pmi':\n",
    "        scored_ngrams = finder.score_ngrams(measures.pmi)\n",
    "    else:\n",
    "        measure='raw_freq'\n",
    "        scored_ngrams = finder.score_ngrams(measures.raw_freq)\n",
    "\n",
    "    df_ngrams = pd.DataFrame(scored_ngrams, columns=[words_colname, measure.replace(\"_\",' ').title()])\n",
    "    if top_n is not None:\n",
    "        return df_ngrams.head(top_n)\n",
    "    else:\n",
    "        return df_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6e35e-d60a-47be-9062-af5fe4813a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function to find frequency values for quad-grams in low reviews\n",
    "get_ngram_measures_finder(low_reviews_tokens_exploded_list, ngrams=4, top_n=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205076b-4538-420a-9d42-29c434fb86c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3c107-cd94-465e-8af4-0aca71ef7ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fc7fc-eeae-4e27-82ac-9c1c216966cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c660342-23bb-45ac-a1e4-ff50f1ed4efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45d0a0-42f6-4f5f-9a7e-c252eee5927e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee222a44-b4b5-4314-bada-7a078b5d4e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26525ad-771b-4f5d-9670-79d51f39ab1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0f63d-e71c-4cbc-8b0e-6f9f256f2b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd6d60-f0a7-4c20-aafd-b54021dbb85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec05c3-7eb1-4328-90dc-eb407f31d922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
